{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Connect to the database\n",
        "db_path = '/content/sample_data/Blooms.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Get list of tables\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = [row[0] for row in cursor.fetchall()]\n",
        "\n",
        "print(\"Table Summary (One row per table):\\n\")\n",
        "print(f\"{'Table Name':<12} | {'Row Count':<10} | Columns\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# For each table, get column names and row count\n",
        "for table in tables:\n",
        "    # Get row count\n",
        "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
        "    row_count = cursor.fetchone()[0]\n",
        "\n",
        "    # Get column names\n",
        "    cursor.execute(f\"PRAGMA table_info({table});\")\n",
        "    columns = [col[1] for col in cursor.fetchall()]\n",
        "    column_str = ', '.join(columns)\n",
        "\n",
        "    # Display in row\n",
        "    print(f\"{table:<12} | {row_count:<10} | {column_str}\")\n",
        "\n",
        "# Close connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "actsPhXT70rY",
        "outputId": "cb9b07f7-f018-448a-afd3-4d6747f5b4a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table Summary (One row per table):\n",
            "\n",
            "Table Name   | Row Count  | Columns\n",
            "--------------------------------------------------------------------------------\n",
            "FlanCSV      | 100        | resume_path, person_name, current_role, total_experience, current_tenure, certification_count, publication_count, skill_density\n",
            "GPTCSV       | 100        | resume_path, person_name, current_role, total_experience, current_tenure, certification_count, publication_count, skill_density\n",
            "LLamaCSV     | 100        | resume_path, person_name, current_role, total_experience, current_tenure, certification_count, publication_count, skill_density\n",
            "NLPCSV       | 100        | resume_path, person_name, current_role, total_experience, current_tenure, certification_count, publication_count, skill_density\n",
            "GTCSV        | 100        | resume_path, person_name, current_role, total_experience, current_tenure, certification_count, publication_count, skill_density\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your database file\n",
        "db_path = \"/content/sample_data/Blooms.db\"\n",
        "\n",
        "# Connect and read the NLP table\n",
        "conn = sqlite3.connect(db_path)\n",
        "nlp_df = pd.read_sql_query(\"SELECT * FROM NLPCSV\", conn)\n",
        "conn.close()\n",
        "\n",
        "# Display top rows\n",
        "print(\"ðŸ“„ NLP Resume Table from Database:\")\n",
        "nlp_df.head(10)\n"
      ],
      "metadata": {
        "id": "qSUdQ9COc1el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKoriAao0THT",
        "outputId": "d436ca9e-7b94-4fc1-a393-67b32c329821"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step1_to_3_extract_jd_features.py\n",
        "\n",
        "from docx import Document\n",
        "import os\n",
        "import re\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"Extracts all non-empty lines from a .docx file.\"\"\"\n",
        "    doc = Document(file_path)\n",
        "    return \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "def extract_jd_features(jd_text):\n",
        "    \"\"\"Extracts structured features from raw JD text.\"\"\"\n",
        "    jd_text_lower = jd_text.lower()\n",
        "    lines = jd_text.strip().splitlines()\n",
        "\n",
        "    # Extract target role (first non-empty line)\n",
        "    target_role = next((line.strip() for line in lines if line.strip()), \"Unknown\")\n",
        "\n",
        "    # Extract minimum experience (fallback = 1)\n",
        "    exp_match = re.search(r'(\\d+)\\+?\\s+years? of experience', jd_text_lower)\n",
        "    min_experience = int(exp_match.group(1)) if exp_match else 1\n",
        "\n",
        "    # Define skill keywords\n",
        "    skill_keywords = [\n",
        "        \"python\", \"sql\", \"excel\", \"communication\", \"leadership\",\n",
        "        \"project management\", \"data analysis\", \"crm\", \"cold calling\"\n",
        "    ]\n",
        "\n",
        "    expected_skills = [skill for skill in skill_keywords if skill in jd_text_lower]\n",
        "    skill_density = len(expected_skills)\n",
        "\n",
        "    # Binary flags\n",
        "    cert_required = \"certification\" in jd_text_lower\n",
        "    pub_required = \"publication\" in jd_text_lower or \"research paper\" in jd_text_lower\n",
        "\n",
        "    return {\n",
        "        \"expected_skills\": expected_skills,\n",
        "        \"cert_required\": cert_required,\n",
        "        \"pub_required\": pub_required,\n",
        "        \"min_experience\": min_experience,\n",
        "        \"target_role\": target_role\n",
        "    }\n",
        "\n",
        "# === RUN THIS MODULE INTERACTIVELY ===\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    jd_file_path = input(\"ðŸ“‚ Enter full path to your JD (.docx) file: \").strip()\n",
        "\n",
        "    if not os.path.exists(jd_file_path):\n",
        "        print(\"âŒ File does not exist. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    jd_text = extract_text_from_docx(jd_file_path)\n",
        "    print(\"\\nðŸ“„ Extracted JD Text:\\n\")\n",
        "    print(jd_text[:1000] + \"\\n...\" if len(jd_text) > 1000 else jd_text)\n",
        "\n",
        "    jd_features = extract_jd_features(jd_text)\n",
        "\n",
        "    print(\"\\nâœ… Extracted JD Features for AHP:\")\n",
        "    for k, v in jd_features.items():\n",
        "        print(f\"{k:<18}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MXFT0X7ejLX",
        "outputId": "9552d2f4-cdf0-4117-db19-cf48db6057e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ Enter full path to your JD (.docx) file: /content/sample_data/Business Development Executive.docx\n",
            "\n",
            "ðŸ“„ Extracted JD Text:\n",
            "\n",
            "Business Development Executive\n",
            "About the Position\n",
            "We are seeking proactive and driven Business Development Executives to join our admissions team. This role is instrumental in promoting our specialized Postgraduate, Masterâ€™s, and Executive Certification Programs designed for working professionals across industries. You will work alongside the Business Development Manager to generate leads, engage with prospective learners, and drive admissions conversions.\n",
            "Job Role and Responsibilities\n",
            "Proactively identify and engage potential candidates via inbound/outbound calls, cold calling, and email campaigns\n",
            "Qualify leads based on eligibility and interest, and collaborate with the Business Development Manager for timely follow-ups and conversions\n",
            "Recommend suitable programs to prospective learners based on their professional background and career goals\n",
            "Maintain detailed and up-to-date records of all interactions and lead progress using the CRM system\n",
            "Ensure a minimum of 180 minutes of daily talk\n",
            "...\n",
            "\n",
            "âœ… Extracted JD Features for AHP:\n",
            "expected_skills   : ['excel', 'communication', 'crm', 'cold calling']\n",
            "cert_required     : True\n",
            "pub_required      : False\n",
            "min_experience    : 1\n",
            "target_role       : Business Development Executive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step4_compute_fitment.py\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# === AHP Setup ===\n",
        "pairwise_matrix = np.array([\n",
        "    [1,     4,     3,     5,     3,     4],     # experience\n",
        "    [1/4,   1,     1/2,   2,     1/2,   1],     # certification\n",
        "    [1/3,   2,     1,     3,     1,     2],     # tenure\n",
        "    [1/5,   1/2,   1/3,   1,     1/3,   1],     # publication\n",
        "    [1/3,   2,     1,     3,     1,     2],     # skill_density\n",
        "    [1/4,   1,     1/2,   1,     1/2,   1]      # role_match\n",
        "])\n",
        "\n",
        "criteria = ['experience', 'certification', 'tenure', 'publication', 'skill_density', 'role_match']\n",
        "\n",
        "def compute_ahp_weights(matrix, labels):\n",
        "    eigvals, eigvecs = np.linalg.eig(matrix)\n",
        "    max_index = np.argmax(eigvals.real)\n",
        "    principal_eigvec = eigvecs[:, max_index].real\n",
        "    normalized_weights = principal_eigvec / principal_eigvec.sum()\n",
        "    return dict(zip(labels, np.round(normalized_weights, 4)))\n",
        "\n",
        "weights = compute_ahp_weights(pairwise_matrix, criteria)\n",
        "\n",
        "# === JD Features (Set this for your JD) ===\n",
        "jd_features = {\n",
        "    \"expected_skills\": [\"python\", \"machine learning\", \"sql\"],\n",
        "    \"cert_required\": True,\n",
        "    \"pub_required\": False,\n",
        "    \"min_experience\": 2,\n",
        "    \"target_role\": \"data analyst\"\n",
        "}\n",
        "\n",
        "def compute_ahp_fitment_score(resume_features, jd_features, weights):\n",
        "    resume_skills = set([s.lower() for s in resume_features.get('skills', [])]) if resume_features.get('skills') else set()\n",
        "    expected_skills = set([s.lower() for s in jd_features.get('expected_skills', [])])\n",
        "    S = len(resume_skills & expected_skills) / len(expected_skills) if expected_skills else 0\n",
        "    C = 1 if jd_features['cert_required'] and resume_features.get('certification_count', 0) > 0 else 0\n",
        "    P = 1 if jd_features['pub_required'] and resume_features.get('publication_count', 0) > 0 else 0\n",
        "    E = min(resume_features.get('total_experience', 0) / jd_features['min_experience'], 1) if jd_features['min_experience'] > 0 else 1\n",
        "    T = min(resume_features.get('current_tenure', 0) / 1, 1)\n",
        "    R = SequenceMatcher(None, resume_features.get('current_role', '').lower(), jd_features.get('target_role', '').lower()).ratio()\n",
        "    score = (\n",
        "        weights['experience'] * E +\n",
        "        weights['certification'] * C +\n",
        "        weights['tenure'] * T +\n",
        "        weights['publication'] * P +\n",
        "        weights['skill_density'] * S +\n",
        "        weights['role_match'] * R\n",
        "    )\n",
        "    return int(round(score * 100))\n",
        "\n",
        "def classify_fit(score):\n",
        "    if score < 31:\n",
        "        return \"Low Fit\"\n",
        "    elif score < 71:\n",
        "        return \"Medium Fit\"\n",
        "    else:\n",
        "        return \"Fit\"\n",
        "\n",
        "# === Fitment CSV Generation ===\n",
        "DB_PATH = r\"/content/sample_data/Blooms.db\"\n",
        "OUTPUT_DIR = \"/content/sample_data/fitment_reports\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "model_tables = [\"FlanCSV\", \"GPTCSV\", \"LLamaCSV\", \"NLPCSV\", \"GTCSV\"]\n",
        "\n",
        "def parse_skills_field(field):\n",
        "    try:\n",
        "        # Assuming a comma-separated skill string or list-like string\n",
        "        if isinstance(field, str):\n",
        "            return [s.strip() for s in field.split(\",\")]\n",
        "        elif isinstance(field, list):\n",
        "            return field\n",
        "    except:\n",
        "        return []\n",
        "    return []\n",
        "\n",
        "def process_table(table, conn, jd_features, weights):\n",
        "    df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
        "\n",
        "    required_cols = {\n",
        "        \"resume_path\", \"person_name\", \"total_experience\", \"current_tenure\",\n",
        "        \"certification_count\", \"publication_count\", \"skill_density\", \"current_role\"\n",
        "    }\n",
        "\n",
        "    if not required_cols.issubset(df.columns):\n",
        "        print(f\"âš ï¸ Skipping {table} â€” missing required columns.\")\n",
        "        return\n",
        "\n",
        "    # Parse skills if available\n",
        "    if \"skills\" in df.columns:\n",
        "        df[\"skills_list\"] = df[\"skills\"].apply(parse_skills_field)\n",
        "    else:\n",
        "        df[\"skills_list\"] = [[] for _ in range(len(df))]\n",
        "\n",
        "    df[\"fitment_score\"] = df.apply(\n",
        "        lambda row: compute_ahp_fitment_score(row.to_dict(), jd_features, weights), axis=1\n",
        "    )\n",
        "    df[\"fitment_level\"] = df[\"fitment_score\"].apply(classify_fit)\n",
        "\n",
        "    out_df = df[[\"resume_path\", \"person_name\", \"fitment_level\", \"fitment_score\"]]\n",
        "    out_file = os.path.join(OUTPUT_DIR, f\"{table}_AHP_fitment.csv\")\n",
        "    out_df.to_csv(out_file, index=False)\n",
        "    print(f\"âœ… Saved: {out_file}\")\n",
        "\n",
        "# === Run for all model tables ===\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "print(\"\\nðŸ“Š Computing AHP-based fitment for all models...\\n\")\n",
        "for table in model_tables:\n",
        "    process_table(table, conn, jd_features, weights)\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KPJ-3ec1C_1",
        "outputId": "e818f3b6-7dc5-49b6-9773-52a55bd8dc3d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Computing AHP-based fitment for all models...\n",
            "\n",
            "âœ… Saved: /content/sample_data/fitment_reports/FlanCSV_AHP_fitment.csv\n",
            "âœ… Saved: /content/sample_data/fitment_reports/GPTCSV_AHP_fitment.csv\n",
            "âœ… Saved: /content/sample_data/fitment_reports/LLamaCSV_AHP_fitment.csv\n",
            "âœ… Saved: /content/sample_data/fitment_reports/NLPCSV_AHP_fitment.csv\n",
            "âœ… Saved: /content/sample_data/fitment_reports/GTCSV_AHP_fitment.csv\n"
          ]
        }
      ]
    }
  ]
}