{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy spacy thinc pyresparser ahpy --quiet\n"
      ],
      "metadata": {
        "id": "Gp9Ioqd5BipU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and install all compatible packages\n",
        "!pip uninstall -y numpy spacy thinc --quiet\n",
        "!pip install thinc==7.4.5 --quiet\n",
        "!pip install pyresparser==1.0.6 nltk==3.6.7 python-docx ahpy --quiet\n"
      ],
      "metadata": {
        "id": "OEECUI3_Bqa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy spacy thinc\n"
      ],
      "metadata": {
        "id": "V6szqObABsOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compatible numpy version for TensorFlow, OpenCV, etc.\n",
        "!pip install numpy==1.26.4\n",
        "\n",
        "# Compatible spaCy and thinc versions\n",
        "!pip install spacy==3.7.4 thinc==8.2.2\n"
      ],
      "metadata": {
        "id": "u_UQ_OSNBvkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "import spacy\n",
        "spacy.cli.download(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "wI_GluCBBxx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF --quiet"
      ],
      "metadata": {
        "id": "QNLF9XjaB0g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OCR dependencies\n",
        "!pip install pdf2image pytesseract Pillow --quiet\n",
        "!apt-get install poppler-utils -y\n"
      ],
      "metadata": {
        "id": "LcJ2tkFvB14r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "import re\n",
        "import docx\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from difflib import SequenceMatcher\n",
        "from pyresparser import ResumeParser\n",
        "import string\n",
        "import random\n",
        "\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "ro06HT3cacqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "print(\"spaCy loaded:\", spacy.blank(\"en\"))\n"
      ],
      "metadata": {
        "id": "AjstxifcB5Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Paths ---\n",
        "zip_path = \"/content/drive/MyDrive/Total_Resumes.zip\"\n",
        "extract_to = \"/content/resumes_unzipped\"\n",
        "resume_folder = os.path.join(extract_to, \"Total_Resumes\")\n",
        "\n",
        "# --- Unzip resumes ---\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "# --- Collect only PDF resumes ---\n",
        "pdf_files = [\n",
        "    os.path.join(resume_folder, f)\n",
        "    for f in os.listdir(resume_folder)\n",
        "    if f.lower().endswith(\".pdf\")\n",
        "]\n",
        "\n",
        "# --- Extract Text Functions ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "\n",
        "    if not text.strip():\n",
        "        print(f\"No text extracted from {pdf_path}, trying OCR...\")\n",
        "        text = extract_text_with_ocr(pdf_path)\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_text_with_ocr(pdf_path):\n",
        "    try:\n",
        "        pages = convert_from_path(pdf_path)\n",
        "        ocr_text = \"\"\n",
        "        for page in pages:\n",
        "            ocr_text += pytesseract.image_to_string(page)\n",
        "        return ocr_text\n",
        "    except Exception as e:\n",
        "        print(f\"OCR failed for {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# --- Strict Role Extraction (as in Shiny) ---\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "\n",
        "# List of plausible job title keywords\n",
        "job_keywords = [\n",
        "    \"engineer\", \"developer\", \"manager\", \"analyst\", \"consultant\", \"specialist\", \"coordinator\",\n",
        "    \"executive\", \"associate\", \"administrator\", \"advisor\", \"officer\", \"lead\", \"head\", \"director\",\n",
        "    \"scientist\", \"architect\", \"designer\", \"supervisor\", \"technician\", \"trainer\", \"assistant\"\n",
        "]\n",
        "\n",
        "fallback_titles = [\n",
        "    \"Engineer\", \"Analyst\", \"Developer\", \"Executive\", \"Consultant\", \"Manager\", \"Data Scientist\", \"Software Engineer\"\n",
        "]\n",
        "# --- Strict Role Extraction (as in Shiny) ---\n",
        "invalid_role_keywords = [\n",
        "    # Degrees/fields\n",
        "    'b.tech', 'btech', 'b.e', 'be', 'bachelor', 'master', 'msc', 'm.sc', 'mba', 'pgdm', 'phd',\n",
        "    'engineering', 'computer science', 'information technology', 'electronics', 'civil', 'mechanical',\n",
        "    # Section headers\n",
        "    'education', 'skills', 'projects', 'experience', 'certifications', 'achievements', 'summary',\n",
        "    'objective', 'profile', 'about me', 'contact', 'personal details',\n",
        "    # Soft skills/qualities\n",
        "    'leadership', 'leadership qualities', 'leadership quality', 'communication', 'team player',\n",
        "    'problem solving', 'quick learner', 'hardworking', 'presentation skills', 'good communication',\n",
        "    # Formatting artifacts\n",
        "    '‚Ä¢', 'ÔÇ∑', 'ÔÄ†', '-', '_', '‚Äî', '‚Äì', '‚Ä¢', '¬∑', '‚Ä¶', '‚Äú', '‚Äù', '‚Äò', '‚Äô',\n",
        "    # Company names (add your own if needed)\n",
        "    'leadsquared',\n",
        "    # Misc\n",
        "    'simply', 'trainee', 'intern', 'fresher'\n",
        "]\n",
        "\n",
        "\n",
        "def is_valid_role(line):\n",
        "    l = line.lower().strip()\n",
        "    l = l.strip(string.punctuation + \"‚Ä¢ÔÇ∑ÔÄ†-‚Äî‚Äì¬∑‚Ä¶‚Äú‚Äù‚Äò‚Äô\")\n",
        "    # Exclude lines with academic titles or references\n",
        "    if any(x in l for x in ['advisor:', 'prof.', 'dr.', 'reference', 'referee', 'guide', 'supervisor']):\n",
        "        return False\n",
        "    if not (1 <= len(l.split()) <= 6):\n",
        "        return False\n",
        "    if any(char.isdigit() for char in l) or \"http\" in l or \"www\" in l:\n",
        "        return False\n",
        "    for kw in invalid_role_keywords:\n",
        "        if kw in l:\n",
        "            return False\n",
        "    if l.isupper() or l.split()[0] in ['education', 'skills', 'projects', 'experience', 'summary', 'objective', 'profile']:\n",
        "        return False\n",
        "    if any(jk in l for jk in job_keywords):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def extract_most_recent_role(text):\n",
        "    date_pattern = re.compile(\n",
        "        r'((?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[\\s\\-.,]*\\d{2,4}|\\d{1,2}/\\d{4})\\s*[-‚Äìto]+\\s*(present|current|(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[\\s\\-.,]*\\d{2,4}|\\d{1,2}/\\d{4})',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "    for i, line in enumerate(lines):\n",
        "        if date_pattern.search(line):\n",
        "            for j in range(1, 3):\n",
        "                if i - j >= 0:\n",
        "                    candidate = lines[i - j]\n",
        "                    for part in re.split(r'\\||,| and |/| at ', candidate):\n",
        "                        part = part.strip()\n",
        "                        if is_valid_role(part):\n",
        "                            return part.title()\n",
        "    for line in lines:\n",
        "        for part in re.split(r'\\||,| and |/| at ', line):\n",
        "            part = part.strip()\n",
        "            if is_valid_role(part):\n",
        "                return part.title()\n",
        "    return random.choice(fallback_titles)\n",
        "\n",
        "def clean_role(role):\n",
        "    if not isinstance(role, str):\n",
        "        return role\n",
        "    # Remove only true prefixes\n",
        "    role = re.sub(r'^(designation:|role:|certificate by:|advisor:)\\s*', '', role, flags=re.I)\n",
        "    # Split on common delimiters and take the first plausible part\n",
        "    for delim in ['|', ',', '&', ' and ', '/', ' at ']:\n",
        "        if delim in role:\n",
        "            parts = [p.strip() for p in role.split(delim)]\n",
        "            for part in parts:\n",
        "                if is_valid_role(part):\n",
        "                    return part.title()\n",
        "            return parts[0].title()\n",
        "    role = role.strip().strip('.')\n",
        "    return role.title()\n",
        "\n",
        "def guess_role_from_text(text):\n",
        "    for line in text.splitlines():\n",
        "        l = line.strip().lower()\n",
        "        if not (1 <= len(l.split()) <= 3):\n",
        "            continue\n",
        "        if any(char.isdigit() for char in l):\n",
        "            continue\n",
        "        if any(k in l for k in invalid_role_keywords):\n",
        "            continue\n",
        "        if any(jk in l for jk in job_keywords):\n",
        "            return line.strip().title()\n",
        "    return random.choice(fallback_titles)\n",
        "\n",
        "\n",
        "\n",
        "def clean_current_role(raw_role, resume_text):\n",
        "    # Use parser's role if valid and not concatenated\n",
        "    if raw_role and isinstance(raw_role, str):\n",
        "        for part in re.split(r'\\||,| and |/| at ', raw_role):\n",
        "            part = part.strip()\n",
        "            if is_valid_role(part):\n",
        "                return clean_role(part)\n",
        "    # Try context-based extraction\n",
        "    context_role = extract_most_recent_role(resume_text)\n",
        "    if context_role:\n",
        "        return clean_role(context_role)\n",
        "    # Fallback\n",
        "    return random.choice(fallback_titles)\n",
        "\n",
        "# --- Resume Feature Extraction ---\n",
        "def extract_resume_features_combined(file_path, resume_text):\n",
        "    lowered = resume_text.lower()\n",
        "    lines = resume_text.splitlines()\n",
        "    date_pattern = re.compile(\n",
        "        r'((?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[\\s\\-.,]*\\d{2,4}|\\d{1,2}/\\d{4})\\s*[-‚Äìto]+\\s*(present|current|(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[\\s\\-.,]*\\d{2,4}|\\d{1,2}/\\d{4})',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    total_months = 0\n",
        "    latest_start = None\n",
        "    job_count = 0\n",
        "\n",
        "    for line in lines:\n",
        "        match = date_pattern.search(line)\n",
        "        if match:\n",
        "            try:\n",
        "                start_str, end_str = match.groups()\n",
        "                start_date = datetime.strptime(start_str.strip()[:3] + start_str.strip()[-4:], '%b%Y') if '/' not in start_str else datetime.strptime(start_str.strip(), '%m/%Y')\n",
        "                end_date = datetime.now() if 'present' in end_str.lower() else datetime.strptime(end_str.strip()[:3] + end_str.strip()[-4:], '%b%Y') if '/' not in end_str else datetime.strptime(end_str.strip(), '%m/%Y')\n",
        "                if start_date <= end_date:\n",
        "                    delta = relativedelta(end_date, start_date)\n",
        "                    months = delta.years * 12 + delta.months\n",
        "                    total_months += months\n",
        "                    job_count += 1\n",
        "                    if latest_start is None or start_date > latest_start:\n",
        "                        latest_start = start_date\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    current_tenure_months = 0\n",
        "    if latest_start:\n",
        "        delta = relativedelta(datetime.now(), latest_start)\n",
        "        current_tenure_months = delta.years * 12 + delta.months\n",
        "\n",
        "    total_exp = int(round(total_months / 12))\n",
        "    current_tenure = int(round(current_tenure_months / 12))\n",
        "\n",
        "    # --- Fallbacks and Consistency ---\n",
        "    parser_exp = 0\n",
        "    current_role = \"\"\n",
        "    skills = []\n",
        "\n",
        "    try:\n",
        "        parsed = ResumeParser(file_path).get_extracted_data()\n",
        "        print(f\"{os.path.basename(file_path)} parser output: {parsed}\")  # Debugging\n",
        "        # --- Role extraction ---\n",
        "        raw_role = parsed.get('designation', \"\")\n",
        "        if isinstance(raw_role, list):\n",
        "            raw_role = raw_role[0] if raw_role else \"\"\n",
        "        role_from_parser = clean_current_role(raw_role, resume_text)\n",
        "        # --- Skills extraction ---\n",
        "        skills = parsed.get('skills', [])\n",
        "        # --- Experience fallback ---\n",
        "        parser_exp = parsed.get('total_experience', 0) or 0\n",
        "    except Exception as e:\n",
        "        print(f\"ResumeParser failed for {os.path.basename(file_path)}: {e}\")\n",
        "        role_from_parser = \"\"\n",
        "        skills = []\n",
        "\n",
        "    # --- Try context-based and fallback role extraction if needed ---\n",
        "    role_from_context = extract_most_recent_role(resume_text)\n",
        "    role_from_guess = guess_role_from_text(resume_text)\n",
        "\n",
        "    # --- Choose the first valid, non-empty role ---\n",
        "    for candidate in [role_from_parser, role_from_context, role_from_guess]:\n",
        "        if candidate and is_valid_role(candidate):\n",
        "            current_role = candidate\n",
        "            break\n",
        "    else:\n",
        "        current_role = random.choice(fallback_titles)\n",
        "\n",
        "    # --- Experience/tenure fallback logic ---\n",
        "    if total_exp == 0 and current_tenure == 0 and parser_exp:\n",
        "        total_exp = int(round(parser_exp))\n",
        "        current_tenure = total_exp\n",
        "\n",
        "    if total_exp == 0 and current_tenure > 0:\n",
        "        total_exp = current_tenure\n",
        "    if current_tenure == 0 and total_exp > 0:\n",
        "        current_tenure = total_exp\n",
        "    if current_tenure > total_exp:\n",
        "        current_tenure = total_exp\n",
        "\n",
        "    # Try to extract from phrases like \"X years of experience\"\n",
        "    if total_exp == 0 and current_tenure == 0:\n",
        "        exp_phrase = re.search(r'(\\d+)\\s+years? of experience', lowered)\n",
        "        if exp_phrase:\n",
        "            total_exp = int(exp_phrase.group(1))\n",
        "            current_tenure = total_exp\n",
        "\n",
        "    # Final log if still zero\n",
        "    if total_exp == 0 and current_tenure == 0:\n",
        "        print(f\"Could not extract experience for {os.path.basename(file_path)}\")\n",
        "    certs = len(re.findall(r'\\b(certification|certificate|certified)\\b', lowered))\n",
        "    pubs = len(re.findall(r'\\b(publication|journal|conference|research)\\b', lowered))\n",
        "    skill_keywords = ['python', 'sql', 'excel', 'communication', 'crm', 'presentation', 'ai', 'cloud']\n",
        "    skill_density = sum(lowered.count(skill) for skill in skill_keywords)\n",
        "\n",
        "    return {\n",
        "        'current_role': current_role,\n",
        "        'total_experience': total_exp,\n",
        "        'current_tenure': current_tenure,\n",
        "        'certification_count': certs,\n",
        "        'publication_count': pubs,\n",
        "        'skill_density': skill_density,\n",
        "        'job_count': job_count,\n",
        "        'skills': skills\n",
        "    }\n",
        "\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "# --- Blacklist to avoid junk names ---\n",
        "blacklist = {\n",
        "    \"curriculum\", \"resume\", \"btech\", \"b.e\", \"be\", \"mba\", \"mca\", \"mtech\", \"cse\", \"ece\", \"eee\",\n",
        "    \"civil\", \"mech\", \"ece\", \"student\", \"intern\", \"fresher\", \"developer\", \"engineer\", \"job\",\n",
        "    \"file\", \"document\", \"doc\", \"new\", \"final\", \"latest\", \"updated\", \"resume1\", \"resume2\"\n",
        "}\n",
        "\n",
        "def extract_person_name(resume_text, resume_path, email=None, parsed_name=None):\n",
        "    def clean_name(name):\n",
        "        name = name.lower().strip()\n",
        "        name = re.sub(r'[^a-z ]+', '', name)  # Remove non-letter chars\n",
        "        name = re.sub(r'\\s+', ' ', name)      # Normalize spaces\n",
        "        name = name.title()\n",
        "        return name\n",
        "\n",
        "    # --- Try parsed name if valid ---\n",
        "    if parsed_name:\n",
        "        name = clean_name(parsed_name)\n",
        "        if name.lower() not in blacklist and len(name.split()) <= 4 and len(name.split()) >= 1:\n",
        "            return name\n",
        "\n",
        "    # --- Try extracting from email ---\n",
        "    if email:\n",
        "        username = email.split(\"@\")[0]\n",
        "        username = username.replace(\".\", \" \").replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "        name = clean_name(username)\n",
        "        parts = name.split()\n",
        "        # Remove digits and blacklist\n",
        "        parts = [p for p in parts if p.lower() not in blacklist and not any(char.isdigit() for char in p)]\n",
        "        if 1 <= len(parts) <= 3:\n",
        "            return \" \".join([p.title() for p in parts])\n",
        "    return \"Unknown\"\n",
        "\n"
      ],
      "metadata": {
        "id": "XRf0zmMgB7ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Connect to new DB ---\n",
        "db_path = \"/content/sample_data/Resumeparser.db\"\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# --- Drop and recreate the parsed_resumes table with additional fields ---\n",
        "cursor.execute(\"DROP TABLE IF EXISTS parsed_resumes\")\n",
        "conn.commit()\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS parsed_resumes (\n",
        "    person_name TEXT,\n",
        "    resume_path TEXT,\n",
        "    current_role TEXT,\n",
        "    total_experience INTEGER,\n",
        "    current_tenure INTEGER,\n",
        "    certification_count INTEGER,\n",
        "    publication_count INTEGER,\n",
        "    skill_density INTEGER,\n",
        "    full_parsed_json TEXT,\n",
        "    fitment_score INTEGER,\n",
        "    fitment_level TEXT,\n",
        "    email TEXT\n",
        ")\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "\n",
        "# --- Parse and store resumes ---\n",
        "results = []\n",
        "\n",
        "for file in tqdm(pdf_files, desc=\"üìÑ Parsing resumes\"):\n",
        "    text = extract_text_from_pdf(file)\n",
        "    if not text.strip():\n",
        "        print(f\"No text extracted from {os.path.basename(file)}\")\n",
        "        continue\n",
        "\n",
        "    features = extract_resume_features_combined(file, text)\n",
        "\n",
        "    try:\n",
        "        parsed = ResumeParser(file).get_extracted_data()\n",
        "        applicant_name = parsed.get(\"name\", \"\")\n",
        "        email = parsed.get(\"email\", \"\")\n",
        "\n",
        "        if not applicant_name:\n",
        "            if email and \"@\" in email:\n",
        "                applicant_name = email.split(\"@\")[0].replace(\".\", \" \").title()\n",
        "    except:\n",
        "        parsed = {}\n",
        "        email = \"\"\n",
        "        applicant_name = os.path.basename(file).split(\"@\")[0].split(\".\")[0].title()\n",
        "\n",
        "    # Insert into new DB including the new 'email' field\n",
        "    cursor.execute(\"\"\"\n",
        "        INSERT INTO parsed_resumes (\n",
        "            person_name, resume_path, current_role,\n",
        "            total_experience, current_tenure,\n",
        "            certification_count, publication_count,\n",
        "            skill_density, full_parsed_json,\n",
        "            fitment_score, fitment_level, email\n",
        "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    \"\"\", (\n",
        "        applicant_name,\n",
        "        file,\n",
        "        features.get('current_role'),\n",
        "        features.get('total_experience'),\n",
        "        features.get('current_tenure'),\n",
        "        features.get('certification_count'),\n",
        "        features.get('publication_count'),\n",
        "        features.get('skill_density'),\n",
        "        json.dumps(parsed),\n",
        "        None,  # fitment_score (to be calculated later)\n",
        "        None,  # fitment_level (to be classified later)\n",
        "        email\n",
        "    ))\n",
        "\n",
        "conn.commit()\n",
        "print(\" All parsed resume data stored in Resumeparser.db.\")\n"
      ],
      "metadata": {
        "id": "Bj0pHhweB-Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# --- Connect to the same database ---\n",
        "db_path = \"/content/sample_data/Resumeparser.db\"\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# --- Check if 'parsed_resumes' table exists ---\n",
        "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
        "print(\"üìã Available tables in DB:\")\n",
        "print(tables)\n",
        "\n",
        "# --- View sample data if the table exists ---\n",
        "if 'parsed_resumes' in tables['name'].values:\n",
        "    df = pd.read_sql(\"SELECT * FROM parsed_resumes LIMIT 5\", conn)\n",
        "    print(\"\\nüìÑ Sample Records from 'parsed_resumes':\")\n",
        "    print(df)\n",
        "else:\n",
        "    print(\"Table 'parsed_resumes' not found.\")\n",
        "\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "vhJLvUbDCBCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to DB\n",
        "conn = sqlite3.connect(\"/content/sample_data/Resumeparser.db\")\n",
        "\n",
        "# Replace 'parsed_resumes' with your actual table name if different\n",
        "table_name = \"parsed_resumes\"\n",
        "\n",
        "# Get table schema\n",
        "df_schema = pd.read_sql(f\"PRAGMA table_info({table_name})\", conn)\n",
        "\n",
        "print(f\"Columns in table '{table_name}':\")\n",
        "print(df_schema[['cid', 'name', 'type']])\n",
        "\n",
        "print(f\"\\n Total fields: {len(df_schema)}\")\n",
        "\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "zMt99PC7CCJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "import docx\n",
        "\n",
        "# --- JD Path ---\n",
        "jd_path = \"/content/sample_data/Business Development Executive.docx\"\n",
        "\n",
        "# --- JD Text Extraction ---\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = docx.Document(docx_path)\n",
        "    return \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "# --- JD Feature Extraction ---\n",
        "def extract_jd_features(jd_text):\n",
        "    jd_text = jd_text.lower()\n",
        "    target_role = jd_text.split('\\n')[0].strip()\n",
        "    cert_required = bool(re.search(r'\\b(certification|certificate|certified)\\b', jd_text))\n",
        "    pub_required = bool(re.search(r'\\b(publication|journal|conference|research)\\b', jd_text))\n",
        "    exp_match = re.search(r'(\\d+)\\s*(?:to|-)?\\s*(\\d*)\\s*years', jd_text)\n",
        "    min_exp = int(exp_match.group(1)) if exp_match else 0\n",
        "    skill_keywords = [\n",
        "        'python', 'sql', 'excel', 'communication', 'crm', 'presentation',\n",
        "        'machine learning', 'ai', 'cloud', 'power bi', 'salesforce',\n",
        "        'project', 'django', 'flask', 'deep learning'\n",
        "    ]\n",
        "    required_skills = [s for s in skill_keywords if s.lower() in jd_text]\n",
        "    return {\n",
        "        'target_role': target_role,\n",
        "        'cert_required': cert_required,\n",
        "        'pub_required': pub_required,\n",
        "        'min_experience': min_exp,\n",
        "        'expected_skills': required_skills\n",
        "    }\n",
        "\n",
        "# --- AHP Matrix ---\n",
        "pairwise_matrix = np.array([\n",
        "    [1,     4,     3,     5,     3,     4],     # experience\n",
        "    [1/4,   1,     1/2,   2,     1/2,   1],     # certification\n",
        "    [1/3,   2,     1,     3,     1,     2],     # tenure\n",
        "    [1/5,   1/2,   1/3,   1,     1/3,   1],     # publication\n",
        "    [1/3,   2,     1,     3,     1,     2],     # skill_density\n",
        "    [1/4,   1,     1/2,   1,     1/2,   1]      # role_match\n",
        "])\n",
        "\n",
        "def compute_ahp_weights(matrix):\n",
        "    eigvals, eigvecs = np.linalg.eig(matrix)\n",
        "    max_index = np.argmax(eigvals.real)\n",
        "    principal_eigvec = eigvecs[:, max_index].real\n",
        "    weights = principal_eigvec / principal_eigvec.sum()\n",
        "    return dict(zip(\n",
        "        ['experience', 'certification', 'tenure', 'publication', 'skill_density', 'role_match'],\n",
        "        np.round(weights, 4)\n",
        "    ))\n",
        "\n",
        "def compute_ahp_fitment_score(resume_features, jd_features, weights):\n",
        "    resume_skills = set([s.lower() for s in resume_features.get('skills', [])]) if resume_features.get('skills') else set()\n",
        "    expected_skills = set([s.lower() for s in jd_features.get('expected_skills', [])])\n",
        "    S = len(resume_skills & expected_skills) / len(expected_skills) if expected_skills else 0\n",
        "    C = 1 if jd_features['cert_required'] and resume_features['certification'] > 0 else 0\n",
        "    P = 1 if jd_features['pub_required'] and resume_features['publication'] > 0 else 0\n",
        "    E = min(resume_features['total_experience'] / jd_features['min_experience'], 1) if jd_features['min_experience'] > 0 else 1\n",
        "    T = min(resume_features.get('current_tenure', 0) / 1, 1)\n",
        "    R = SequenceMatcher(None, resume_features.get('current_role', '').lower(), jd_features.get('target_role', '').lower()).ratio()\n",
        "    score = (\n",
        "        weights['experience'] * E +\n",
        "        weights['certification'] * C +\n",
        "        weights['tenure'] * T +\n",
        "        weights['publication'] * P +\n",
        "        weights['skill_density'] * S +\n",
        "        weights['role_match'] * R\n",
        "    )\n",
        "    return int(round(score * 100))\n",
        "\n",
        "def classify_fit(score):\n",
        "    if score < 31:\n",
        "        return \"Low Fit\"\n",
        "    elif score < 71:\n",
        "        return \"Medium Fit\"\n",
        "    else:\n",
        "        return \"Fit\"\n",
        "\n",
        "# --- Load JD and compute weights ---\n",
        "jd_text = extract_text_from_docx(jd_path)\n",
        "jd_features = extract_jd_features(jd_text)\n",
        "# --- Show Extracted JD Features ---\n",
        "print(\"\\n Extracted JD Keywords and Features:\")\n",
        "for k, v in jd_features.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "weights = compute_ahp_weights(pairwise_matrix)\n",
        "\n",
        "# --- Connect to DB ---\n",
        "conn = sqlite3.connect(\"/content/sample_data/Resumeparser.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# --- Read Data ---\n",
        "df = pd.read_sql(\"SELECT rowid, * FROM parsed_resumes\", conn)\n",
        "print(f\"Loaded {len(df)} resumes\")\n",
        "\n",
        "# --- Compute Scores and Update DB ---\n",
        "for idx, row in df.iterrows():\n",
        "    try:\n",
        "        parsed = json.loads(row['full_parsed_json']) if isinstance(row['full_parsed_json'], str) else row['full_parsed_json']\n",
        "    except:\n",
        "        parsed = {}\n",
        "\n",
        "    resume_features = {\n",
        "        'current_role': row['current_role'],\n",
        "        'total_experience': row['total_experience'],\n",
        "        'current_tenure': row['current_tenure'],\n",
        "        'certification': row['certification_count'],\n",
        "        'publication': row['publication_count'],\n",
        "        'skill_density': row['skill_density'],\n",
        "        'skills': parsed.get(\"skills\", [])\n",
        "    }\n",
        "\n",
        "    fitment_score = compute_ahp_fitment_score(resume_features, jd_features, weights)\n",
        "    fitment_level = classify_fit(fitment_score)\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        UPDATE parsed_resumes\n",
        "        SET fitment_score = ?, fitment_level = ?\n",
        "        WHERE rowid = ?\n",
        "    \"\"\", (fitment_score, fitment_level, row['rowid']))\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "Hnrn2P8QCDxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- AHP Pairwise Matrix Example ---\n",
        "pairwise_matrix = np.array([\n",
        "    [1,     4,     3,     5,     3,     4],     # experience\n",
        "    [1/4,   1,     1/2,   2,     1/2,   1],     # certification\n",
        "    [1/3,   2,     1,     3,     1,     2],     # tenure\n",
        "    [1/5,   1/2,   1/3,   1,     1/3,   1],     # publication\n",
        "    [1/3,   2,     1,     3,     1,     2],     # skill_density\n",
        "    [1/4,   1,     1/2,   1,     1/2,   1]      # role_match\n",
        "])\n",
        "\n",
        "criteria = ['experience', 'certification', 'tenure', 'publication', 'skill_density', 'role_match']\n",
        "\n",
        "def compute_ahp_weights(matrix, labels):\n",
        "    eigvals, eigvecs = np.linalg.eig(matrix)\n",
        "    max_index = np.argmax(eigvals.real)\n",
        "    principal_eigval = eigvals[max_index].real\n",
        "    principal_eigvec = eigvecs[:, max_index].real\n",
        "\n",
        "    normalized_weights = principal_eigvec / principal_eigvec.sum()\n",
        "\n",
        "    print(\"Principal Eigenvalue (Œªmax):\", round(principal_eigval, 4))\n",
        "    print(\"\\nNormalized AHP Weights:\")\n",
        "    for label, weight in zip(labels, normalized_weights):\n",
        "        print(f\"{label:<15}: {weight:.4f}\")\n",
        "\n",
        "    return dict(zip(labels, np.round(normalized_weights, 4)))\n",
        "\n",
        "# Run the solver\n",
        "weights = compute_ahp_weights(pairwise_matrix, criteria)\n"
      ],
      "metadata": {
        "id": "iYXnpBwbCFu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# --- Connect to your DB ---\n",
        "conn = sqlite3.connect(\"/content/sample_data/Resumeparser.db\")\n",
        "\n",
        "# --- Read entire table ---\n",
        "df = pd.read_sql(\"SELECT * FROM parsed_resumes\", conn)\n",
        "\n",
        "# --- Show first few rows ---\n",
        "print(\"üìÑ Preview of parsed_resumes table:\")\n",
        "print(df.head())\n",
        "\n",
        "# --- Show column names ---\n",
        "print(\"\\n Columns in table:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# --- Optional: View summary ---\n",
        "print(\"\\n Row count:\", len(df))\n",
        "\n",
        "# Close connection\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "QkJaQvyCCHxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Fitment Level Counts:\")\n",
        "print(df['fitment_level'].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "OtF5Gr_cCKBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Connect to your SQLite DB\n",
        "db_path = \"/content/sample_data/Resumeparser.db\"\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Only select the columns that exist in your table\n",
        "df = pd.read_sql(\"SELECT person_name, fitment_score FROM parsed_resumes\", conn)\n",
        "conn.close()\n",
        "\n",
        "print(f\"Total resumes loaded: {len(df)}\")\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df[\"fitment_score\"], bins=10, kde=True, color=\"skyblue\", edgecolor=\"black\")\n",
        "\n",
        "plt.title(\"Fitment Score Distribution of 148 Resumes for JD(Business Development Executive)\", fontsize=14)\n",
        "plt.xlabel(\"Fitment Score\", fontsize=12)\n",
        "plt.ylabel(\"Number of Resumes\", fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C7ZDXAUXCMEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFitment Score Stats:\")\n",
        "print(df['fitment_score'].describe())\n",
        "\n",
        "# Optional: print how many scores are in different ranges\n",
        "print(\"\\n Score Range Buckets:\")\n",
        "print(df['fitment_score'].value_counts(bins=[0, 20, 40, 60, 80, 100], sort=False))\n"
      ],
      "metadata": {
        "id": "MOOGIqSkCOJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx matplotlib seaborn\n"
      ],
      "metadata": {
        "id": "NmEWF-vICQmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCXkn6e2CSiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "import docx\n",
        "\n",
        "# --- JD paths ---\n",
        "jd_files = {\n",
        "    \"Business Development Executive\": \"/content/sample_data/Business Development Executive.docx\",\n",
        "    \"Program Coordinator\": \"/content/sample_data/Program Coordinator.docx\",\n",
        "    \"Research Associate\": \"/content/sample_data/Research Associate.docx\"\n",
        "}\n",
        "\n",
        "# --- JD Text Extraction ---\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = docx.Document(docx_path)\n",
        "    return \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "# --- JD Feature Extraction ---\n",
        "def extract_jd_features(jd_text):\n",
        "    jd_text = jd_text.lower()\n",
        "    target_role = jd_text.split('\\n')[0].strip()\n",
        "    cert_required = bool(re.search(r'\\b(certification|certificate|certified)\\b', jd_text))\n",
        "    pub_required = bool(re.search(r'\\b(publication|journal|conference|research)\\b', jd_text))\n",
        "    exp_match = re.search(r'(\\d+)\\s*(?:to|-)?\\s*(\\d*)\\s*years', jd_text)\n",
        "    min_exp = int(exp_match.group(1)) if exp_match else 0\n",
        "    skill_keywords = [\n",
        "        'python', 'sql', 'excel', 'communication', 'crm', 'presentation',\n",
        "        'machine learning', 'ai', 'cloud', 'power bi', 'salesforce',\n",
        "        'project', 'django', 'flask', 'deep learning'\n",
        "    ]\n",
        "    required_skills = [s for s in skill_keywords if s.lower() in jd_text]\n",
        "    return {\n",
        "        'target_role': target_role,\n",
        "        'cert_required': cert_required,\n",
        "        'pub_required': pub_required,\n",
        "        'min_experience': min_exp,\n",
        "        'expected_skills': required_skills\n",
        "    }\n",
        "\n",
        "# --- AHP ---\n",
        "pairwise_matrix = np.array([\n",
        "    [1,     4,     3,     5,     3,     4],\n",
        "    [1/4,   1,     1/2,   2,     1/2,   1],\n",
        "    [1/3,   2,     1,     3,     1,     2],\n",
        "    [1/5,   1/2,   1/3,   1,     1/3,   1],\n",
        "    [1/3,   2,     1,     3,     1,     2],\n",
        "    [1/4,   1,     1/2,   1,     1/2,   1]\n",
        "])\n",
        "\n",
        "def compute_ahp_weights(matrix):\n",
        "    eigvals, eigvecs = np.linalg.eig(matrix)\n",
        "    max_index = np.argmax(eigvals.real)\n",
        "    principal_eigvec = eigvecs[:, max_index].real\n",
        "    weights = principal_eigvec / principal_eigvec.sum()\n",
        "    return dict(zip(\n",
        "        ['experience', 'certification', 'tenure', 'publication', 'skill_density', 'role_match'],\n",
        "        np.round(weights, 4)\n",
        "    ))\n",
        "\n",
        "def compute_ahp_fitment_score(resume_features, jd_features, weights):\n",
        "    resume_skills = set([s.lower() for s in resume_features.get('skills', [])]) if resume_features.get('skills') else set()\n",
        "    expected_skills = set([s.lower() for s in jd_features.get('expected_skills', [])])\n",
        "    S = len(resume_skills & expected_skills) / len(expected_skills) if expected_skills else 0\n",
        "    C = 1 if jd_features['cert_required'] and resume_features['certification'] > 0 else 0\n",
        "    P = 1 if jd_features['pub_required'] and resume_features['publication'] > 0 else 0\n",
        "    E = min(resume_features['total_experience'] / jd_features['min_experience'], 1) if jd_features['min_experience'] > 0 else 1\n",
        "    T = min(resume_features.get('current_tenure', 0) / 1, 1)\n",
        "    R = SequenceMatcher(None, resume_features.get('current_role', '').lower(), jd_features.get('target_role', '').lower()).ratio()\n",
        "    score = (\n",
        "        weights['experience'] * E +\n",
        "        weights['certification'] * C +\n",
        "        weights['tenure'] * T +\n",
        "        weights['publication'] * P +\n",
        "        weights['skill_density'] * S +\n",
        "        weights['role_match'] * R\n",
        "    )\n",
        "    return int(round(score * 100))\n",
        "\n",
        "# --- Load Resumes from DB ---\n",
        "conn = sqlite3.connect(\"/content/sample_data/Resumeparser.db\")\n",
        "df_all = pd.read_sql(\"SELECT * FROM parsed_resumes\", conn)\n",
        "\n",
        "# --- Compute Scores for Each JD ---\n",
        "weights = compute_ahp_weights(pairwise_matrix)\n",
        "jd_dfs = []\n",
        "\n",
        "for jd_name, jd_path in jd_files.items():\n",
        "    jd_text = extract_text_from_docx(jd_path)\n",
        "    jd_features = extract_jd_features(jd_text)\n",
        "\n",
        "    temp_df = df_all.copy()\n",
        "    scores = []\n",
        "    for _, row in temp_df.iterrows():\n",
        "        try:\n",
        "            parsed = json.loads(row['full_parsed_json']) if isinstance(row['full_parsed_json'], str) else {}\n",
        "        except:\n",
        "            parsed = {}\n",
        "        resume_features = {\n",
        "            'current_role': row['current_role'],\n",
        "            'total_experience': row['total_experience'],\n",
        "            'current_tenure': row['current_tenure'],\n",
        "            'certification': row['certification_count'],\n",
        "            'publication': row['publication_count'],\n",
        "            'skill_density': row['skill_density'],\n",
        "            'skills': parsed.get(\"skills\", [])\n",
        "        }\n",
        "        score = compute_ahp_fitment_score(resume_features, jd_features, weights)\n",
        "        scores.append(score)\n",
        "\n",
        "    temp_df[\"fitment_score\"] = scores\n",
        "    temp_df[\"JD\"] = jd_name\n",
        "    jd_dfs.append(temp_df[[\"person_name\", \"skill_density\", \"fitment_score\", \"JD\"]])\n",
        "\n",
        "conn.close()\n",
        "\n",
        "# Final merged DataFrame\n",
        "combined_df = pd.concat(jd_dfs, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "HXJzcUowtkTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bin skill_density for smoother lines (optional)\n",
        "combined_df['skill_bin'] = pd.cut(combined_df['skill_density'], bins=10)\n",
        "\n",
        "# Group by JD and skill_bin to get mean fitment_score\n",
        "line_data = (\n",
        "    combined_df\n",
        "    .groupby(['JD', 'skill_bin'], observed=True)['fitment_score']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Convert bin labels to mid-points for plotting\n",
        "line_data['skill_bin_mid'] = line_data['skill_bin'].apply(lambda x: x.mid)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(\n",
        "    data=line_data,\n",
        "    x='skill_bin_mid',\n",
        "    y='fitment_score',\n",
        "    hue='JD',\n",
        "    palette=[\"#003f5c\", \"#ffa600\", \"#7f4f24\"],  # dark blue, orange, brown\n",
        "    linewidth=2.5,\n",
        "    marker='o'\n",
        ")\n",
        "\n",
        "plt.title(\"Average AHP Fitment Score by Skill Density\", fontsize=14)\n",
        "plt.xlabel(\"Skill Density\", fontsize=12)\n",
        "plt.ylabel(\"Fitment Score (AHP)\", fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.legend(title=\"Job Description\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mveV9jVOCUo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Load full parsed resumes from DB\n",
        "conn = sqlite3.connect(\"/content/sample_data/Resumeparser.db\")\n",
        "df_all = pd.read_sql(\"SELECT * FROM parsed_resumes\", conn)\n",
        "conn.close()\n",
        "\n",
        "# Work on full resume data\n",
        "df = df_all.copy()\n",
        "\n",
        "# Features to normalize\n",
        "expected_columns = [\n",
        "    \"total_experience\",\n",
        "    \"current_tenure\",\n",
        "    \"certification_count\",\n",
        "    \"publication_count\",\n",
        "    \"skill_density\"\n",
        "]\n",
        "\n",
        "# Check for missing columns\n",
        "missing_cols = [col for col in expected_columns if col not in df.columns]\n",
        "if missing_cols:\n",
        "    raise KeyError(f\" Missing columns in df: {missing_cols}\")\n",
        "\n",
        "# Binary normalize each feature\n",
        "for col in expected_columns:\n",
        "    max_val = df[col].max()\n",
        "    if max_val > 0:\n",
        "        df[f\"norm_{col}\"] = (df[col] / max_val >= 0.5).astype(int)\n",
        "    else:\n",
        "        df[f\"norm_{col}\"] = 0\n",
        "\n",
        "# Preview results\n",
        "print(\"\\n Normalized Columns Preview:\")\n",
        "print(df[[f\"norm_{col}\" for col in expected_columns]].head())\n"
      ],
      "metadata": {
        "id": "XMXZsUVACXCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U shiny"
      ],
      "metadata": {
        "id": "0yehuoLECZE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4\n"
      ],
      "metadata": {
        "id": "Mn2o0DC6Catx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shiny\n",
        "print(shiny.__version__)\n"
      ],
      "metadata": {
        "id": "mcO3tACKCcOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Restart and clean old processes\n",
        "!pkill -f shiny\n",
        "!pkill -f cloudflared\n",
        "#   shiny_code = r\"\"\""
      ],
      "metadata": {
        "id": "EzsBTm2mQpLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_config.py\n",
        "import sqlite3\n",
        "import ahpy\n",
        "\n",
        "# Database Path\n",
        "DB_PATH = \"/content/sample_data/Resumeparser.db\"\n",
        "conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "\n",
        "#  Keywords and Role Helpers\n",
        "irrelevant_keywords = [\n",
        "    'home town', 'education certifications', 'certifications', 'certification', 'education', 'summary',\n",
        "    'objective', 'personal', 'contact', 'details', 'profile', 'curriculum', 'vitae', 'resume', 'skills',\n",
        "    'hobbies', 'interests', 'languages', 'declaration', 'reference', 'references', 'address', 'email',\n",
        "    'phone', 'mobile', 'dob', 'date of birth'\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "job_keywords = [\n",
        "    \"engineer\", \"developer\", \"manager\", \"analyst\", \"consultant\", \"specialist\", \"coordinator\",\n",
        "    \"executive\", \"associate\", \"administrator\", \"advisor\", \"officer\", \"lead\", \"head\", \"director\",\n",
        "    \"scientist\", \"architect\", \"designer\", \"supervisor\", \"technician\", \"trainer\", \"assistant\"\n",
        "]\n",
        "\n",
        "fallback_titles = [\"Engineer\", \"Analyst\", \"Developer\", \"Consultant\"]\n",
        "\n",
        "#  Skills to scan from resumes & JDs\n",
        "skill_keywords = ['python', 'sql', 'excel', 'communication', 'crm', 'presentation', 'ai', 'cloud']\n",
        "\n",
        "# AHP Pairwise Comparison Matrix\n",
        "pairwise = {\n",
        "    ('experience', 'certification'): 3,\n",
        "    ('experience', 'tenure'): 2,\n",
        "    ('experience', 'skill_density'): 2,\n",
        "    ('experience', 'publication'): 4,\n",
        "    ('experience', 'role_match'): 2,\n",
        "    ('certification', 'tenure'): 1,\n",
        "    ('certification', 'skill_density'): 0.5,\n",
        "    ('certification', 'publication'): 2,\n",
        "    ('certification', 'role_match'): 1,\n",
        "    ('tenure', 'skill_density'): 0.5,\n",
        "    ('tenure', 'publication'): 2,\n",
        "    ('tenure', 'role_match'): 1,\n",
        "    ('skill_density', 'publication'): 3,\n",
        "    ('skill_density', 'role_match'): 1,\n",
        "    ('publication', 'role_match'): 0.5,\n",
        "}\n",
        "\n",
        "criteria = ahpy.Compare(\"Fitment\", pairwise)\n",
        "default_ahp_weights = criteria.target_weights.copy()  # Used when no manual slider input\n",
        "\n",
        "#  UI Column Mapping\n",
        "col_map = {\n",
        "    \"person_name\": \"Name\",\n",
        "    \"current_role\": \"Current Role\",\n",
        "    \"total_experience\": \"Experience\",\n",
        "    \"current_tenure\": \"Tenure\",\n",
        "    \"certification_count\": \"Certifications\",\n",
        "    \"publication_count\": \"Publications\",\n",
        "    \"skill_density\": \"Skills\",\n",
        "    \"fitment_score\": \"Score\",\n",
        "    \"fitment_level\": \"Level\",\n",
        "}\n",
        "\n",
        "# These will be normalized later (binary 0/1)\n",
        "features_to_normalize = [\n",
        "    \"total_experience\",\n",
        "    \"current_tenure\",\n",
        "    \"certification_count\",\n",
        "    \"publication_count\",\n",
        "    \"skill_density\"\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF6-mWCrDfpC",
        "outputId": "408c15a4-0da4-435d-c5f2-79548ec90ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile extractors.py\n",
        "import os\n",
        "import re\n",
        "import fitz\n",
        "import docx\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pyresparser import ResumeParser\n",
        "from difflib import SequenceMatcher\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "from app_config import irrelevant_keywords, job_keywords, fallback_titles, skill_keywords, pairwise\n",
        "\n",
        "# ---------- Utility Functions ----------\n",
        "\n",
        "invalid_role_keywords = [\n",
        "    # Degrees/fields\n",
        "    'b.tech', 'btech', 'b.e', 'be', 'bachelor', 'master', 'msc', 'm.sc', 'mba', 'pgdm', 'phd',\n",
        "    'engineering', 'computer science', 'information technology', 'electronics', 'civil', 'mechanical',\n",
        "    # Section headers\n",
        "    'education', 'skills', 'projects', 'experience', 'certifications', 'achievements', 'summary',\n",
        "    'objective', 'profile', 'about me', 'contact', 'personal details',\n",
        "    # Soft skills/qualities\n",
        "    'leadership', 'leadership qualities', 'leadership quality', 'communication', 'team player',\n",
        "    'problem solving', 'quick learner', 'hardworking', 'presentation skills', 'good communication',\n",
        "    # Formatting artifacts\n",
        "    '‚Ä¢', 'ÔÇ∑', 'ÔÄ†', '-', '_', '‚Äî', '‚Äì', '‚Ä¢', '¬∑', '‚Ä¶', '‚Äú', '‚Äù', '‚Äò', '‚Äô',\n",
        "    # Company names (add your own if needed)\n",
        "    'leadsquared',\n",
        "    # Misc\n",
        "    'simply', 'trainee', 'intern', 'fresher','home town', 'education certifications', 'certifications', 'certification', 'education', 'summary',\n",
        "    'objective', 'personal', 'contact', 'details', 'profile', 'curriculum', 'vitae', 'resume', 'skills',\n",
        "    'hobbies', 'interests', 'languages', 'declaration', 'reference', 'references', 'address', 'email',\n",
        "    'phone', 'mobile', 'dob', 'date of birth'\n",
        "]\n",
        "\n",
        "def extract_text_with_ocr(pdf_path):\n",
        "    try:\n",
        "        pages = convert_from_path(pdf_path)\n",
        "        return \"\\n\".join([pytesseract.image_to_string(page) for page in pages])\n",
        "    except Exception as e:\n",
        "        print(f\" OCR failed for {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            text = \"\".join([page.get_text() for page in doc])\n",
        "    except Exception as e:\n",
        "        print(f\" Error reading {pdf_path}: {e}\")\n",
        "    return text.strip() or extract_text_with_ocr(pdf_path)\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = docx.Document(docx_path)\n",
        "    return \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "# ---------- JD Processing ----------\n",
        "def extract_jd_features(jd_text):\n",
        "    jd_text = jd_text.lower()\n",
        "    target_role = jd_text.split('\\n')[0].strip()\n",
        "    cert_required = bool(re.search(r'\\b(certification|certificate|certified)\\b', jd_text))\n",
        "    pub_required = bool(re.search(r'\\b(publication|journal|conference|research)\\b', jd_text))\n",
        "    exp_match = re.search(r'(\\d+)\\s*(?:to|-)?\\s*(\\d*)\\s*years', jd_text)\n",
        "    min_exp = int(exp_match.group(1)) if exp_match else 0\n",
        "    required_skills = [s for s in skill_keywords if s.lower() in jd_text]\n",
        "    return {\n",
        "        'target_role': target_role,\n",
        "        'cert_required': cert_required,\n",
        "        'pub_required': pub_required,\n",
        "        'min_experience': min_exp,\n",
        "        'expected_skills': required_skills\n",
        "    }\n",
        "\n",
        "def process_jd_file(jd_path):\n",
        "    jd_text = extract_text_from_docx(jd_path)\n",
        "    jd_features = extract_jd_features(jd_text)\n",
        "    ahp_weights = compute_ahp_weights(pairwise_matrix)\n",
        "    return jd_text, jd_features, ahp_weights\n",
        "\n",
        "# ---------- AHP Weight Calculation ----------\n",
        "pairwise_matrix = np.array([\n",
        "    [1,     4,     3,     5,     3,     4],\n",
        "    [1/4,   1,     1/2,   2,     1/2,   1],\n",
        "    [1/3,   2,     1,     3,     1,     2],\n",
        "    [1/5,   1/2,   1/3,   1,     1/3,   1],\n",
        "    [1/3,   2,     1,     3,     1,     2],\n",
        "    [1/4,   1,     1/2,   1,     1/2,   1]\n",
        "])\n",
        "\n",
        "def compute_ahp_weights(matrix):\n",
        "    eigvals, eigvecs = np.linalg.eig(matrix)\n",
        "    max_index = np.argmax(eigvals.real)\n",
        "    principal_eigvec = eigvecs[:, max_index].real\n",
        "    weights = principal_eigvec / principal_eigvec.sum()\n",
        "    return dict(zip(\n",
        "        ['experience', 'certification', 'tenure', 'publication', 'skill_density', 'role_match'],\n",
        "        np.round(weights, 4)\n",
        "    ))\n",
        "\n",
        "# ---------- Role Extraction (Same as Before) ----------\n",
        "def is_irrelevant_role(role):\n",
        "    if not role:\n",
        "        return True\n",
        "    role_lower = role.lower().strip()\n",
        "    if role_lower in irrelevant_keywords:\n",
        "        return True\n",
        "    if len(role_lower) < 3 or len(role_lower) > 40:\n",
        "        return True\n",
        "    if re.fullmatch(r'\\d+', role_lower):\n",
        "        return True\n",
        "    if not any(k in role_lower for k in job_keywords):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def clean_designation(role):\n",
        "    if not role or not isinstance(role, str):\n",
        "        return \"\"\n",
        "    role = re.sub(r'\\b(consultant|intern|trainee|project|fresher|certified|course|college|university|student|degree|btech|engineering)\\b', '', role, flags=re.I)\n",
        "    role = re.sub(r'[-@|/]\\s*\\w+$', '', role)\n",
        "    role = re.sub(r'\\b(and|with|for|in|of|from)$', '', role.strip(), flags=re.I)\n",
        "    role = re.sub(r'[^a-zA-Z\\s\\-]', '', role)\n",
        "    role = re.sub(r'\\s+', ' ', role).strip()\n",
        "    words = role.split()\n",
        "    if 1 <= len(words) <= 5:\n",
        "        return role.title()\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def guess_role_from_lines(text):\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not (1 <= len(line.split()) <= 5):\n",
        "            continue\n",
        "        if any(k in line.lower() for k in job_keywords):\n",
        "            return line.title()\n",
        "    return random.choice(fallback_titles)\n",
        "\n",
        "def extract_most_recent_role(text):\n",
        "    date_pattern = re.compile(\n",
        "        r'((?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[\\s\\-.,]*\\d{2,4}|\\d{1,2}/\\d{4})\\s*[-‚Äìto]+\\s*(present|current|(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[\\s\\-.,]*\\d{2,4}|\\d{1,2}/\\d{4})',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "    for i, line in enumerate(lines):\n",
        "        if date_pattern.search(line):\n",
        "            for j in range(1, 3):\n",
        "                if i - j >= 0:\n",
        "                    candidate = lines[i - j]\n",
        "                    for part in re.split(r'\\||,| and |/| at ', candidate):\n",
        "                        part = part.strip()\n",
        "                        if is_valid_role(part):\n",
        "                            return part.title()\n",
        "    for line in lines:\n",
        "        for part in re.split(r'\\||,| and |/| at ', line):\n",
        "            part = part.strip()\n",
        "            if is_valid_role(part):\n",
        "                return part.title()\n",
        "    return random.choice(fallback_titles)\n",
        "\n",
        "def is_valid_role(line):\n",
        "    l = line.lower().strip()\n",
        "    l = l.strip(string.punctuation + \"‚Ä¢ÔÇ∑ÔÄ†-‚Äî‚Äì¬∑‚Ä¶‚Äú‚Äù‚Äò‚Äô\")\n",
        "    # Exclude lines with academic titles or references\n",
        "    if any(x in l for x in ['advisor:', 'prof.', 'dr.', 'reference', 'referee', 'guide', 'supervisor']):\n",
        "        return False\n",
        "    if not (1 <= len(l.split()) <= 6):\n",
        "        return False\n",
        "    if any(char.isdigit() for char in l) or \"http\" in l or \"www\" in l:\n",
        "        return False\n",
        "    for kw in irrelevant_keywords:\n",
        "        if kw in l:\n",
        "            return False\n",
        "    if l.isupper() or l.split()[0] in ['education', 'skills', 'projects', 'experience', 'summary', 'objective', 'profile']:\n",
        "        return False\n",
        "    if any(jk in l for jk in job_keywords):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def guess_role_from_text(text):\n",
        "    for line in text.splitlines():\n",
        "        l = line.strip().lower()\n",
        "        if not (1 <= len(l.split()) <= 3):\n",
        "            continue\n",
        "        if any(char.isdigit() for char in l):\n",
        "            continue\n",
        "        if any(k in l for k in invalid_role_keywords):\n",
        "            continue\n",
        "        if any(jk in l for jk in job_keywords):\n",
        "            return line.strip().title()\n",
        "    return random.choice(fallback_titles)\n",
        "\n",
        "def extract_designation_from_text(text):\n",
        "    lines = [line.strip() for line in text.splitlines()]\n",
        "    in_experience = False\n",
        "    for line in lines:\n",
        "        if \"professional experience\" in line.lower():\n",
        "            in_experience = True\n",
        "            continue\n",
        "        if in_experience and line:\n",
        "            if len(line.split()) <= 5 and not any(x in line.lower() for x in [\"january\", \"february\", \"2023\", \"2022\", \"present\", \"company\", \"location\", \"@\"]):\n",
        "                return line.strip()\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# ---------- Resume Feature Extraction ----------\n",
        "def extract_resume_features_combined(file_path, resume_text):\n",
        "    lowered = resume_text.lower()\n",
        "    lines = resume_text.splitlines()\n",
        "    date_pattern = re.compile(\n",
        "        r'((?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[\\s\\-.,]*\\d{2,4}|\\d{1,2}/\\d{4})\\s*[-‚Äìto]+\\s*(present|current|(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*[\\s\\-.,]*\\d{2,4}|\\d{1,2}/\\d{4})',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    total_months = 0\n",
        "    latest_start = None\n",
        "    job_count = 0\n",
        "\n",
        "    for line in lines:\n",
        "        match = date_pattern.search(line)\n",
        "        if match:\n",
        "            try:\n",
        "                start_str, end_str = match.groups()\n",
        "                start_date = datetime.strptime(start_str.strip()[:3] + start_str.strip()[-4:], '%b%Y') if '/' not in start_str else datetime.strptime(start_str.strip(), '%m/%Y')\n",
        "                end_date = datetime.now() if 'present' in end_str.lower() else datetime.strptime(end_str.strip()[:3] + end_str.strip()[-4:], '%b%Y') if '/' not in end_str else datetime.strptime(end_str.strip(), '%m/%Y')\n",
        "                if start_date <= end_date:\n",
        "                    delta = relativedelta(end_date, start_date)\n",
        "                    total_months += delta.years * 12 + delta.months\n",
        "                    job_count += 1\n",
        "                    if latest_start is None or start_date > latest_start:\n",
        "                        latest_start = start_date\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    current_tenure_months = 0\n",
        "    if latest_start:\n",
        "        delta = relativedelta(datetime.now(), latest_start)\n",
        "        current_tenure_months = delta.years * 12 + delta.months\n",
        "\n",
        "    total_exp = int(round(total_months / 12))\n",
        "    current_tenure = int(round(current_tenure_months / 12))\n",
        "\n",
        "    current_role = \"\"\n",
        "    skills = []\n",
        "    parser_exp = 0\n",
        "\n",
        "    try:\n",
        "        parsed = ResumeParser(file_path).get_extracted_data()\n",
        "        raw_role = parsed.get('designation', \"\")\n",
        "        if isinstance(raw_role, list):\n",
        "            raw_role = raw_role[0] if raw_role else \"\"\n",
        "        role_from_parser = clean_current_role(raw_role, resume_text)\n",
        "        skills = parsed.get('skills', [])\n",
        "        parser_exp = parsed.get('total_experience', 0) or 0\n",
        "    except Exception as e:\n",
        "        print(f\" ResumeParser failed for {os.path.basename(file_path)}: {e}\")\n",
        "        role_from_parser = \"\"\n",
        "        skills = []\n",
        "\n",
        "    role_from_context = extract_most_recent_role(resume_text)\n",
        "    role_from_guess = guess_role_from_text(resume_text)\n",
        "\n",
        "    for candidate in [role_from_parser, role_from_context, role_from_guess]:\n",
        "        if candidate and is_valid_role(candidate):\n",
        "            current_role = candidate\n",
        "            break\n",
        "    else:\n",
        "        current_role = random.choice(fallback_titles)\n",
        "\n",
        "    if total_exp == 0 and current_tenure == 0 and parser_exp:\n",
        "        total_exp = int(round(parser_exp))\n",
        "        current_tenure = total_exp\n",
        "\n",
        "    if total_exp == 0 and current_tenure > 0:\n",
        "        total_exp = current_tenure\n",
        "    if current_tenure == 0 and total_exp > 0:\n",
        "        current_tenure = total_exp\n",
        "    if current_tenure > total_exp:\n",
        "        current_tenure = total_exp\n",
        "\n",
        "    exp_phrase = re.search(r'(\\d+)\\s+years? of experience', lowered)\n",
        "    if total_exp == 0 and current_tenure == 0 and exp_phrase:\n",
        "        total_exp = int(exp_phrase.group(1))\n",
        "        current_tenure = total_exp\n",
        "\n",
        "    certs = len(re.findall(r'\\b(certification|certificate|certified)\\b', lowered))\n",
        "    pubs = len(re.findall(r'\\b(publication|journal|conference|research)\\b', lowered))\n",
        "    density = sum(lowered.count(skill) for skill in skill_keywords)\n",
        "\n",
        "    return {\n",
        "        'current_role': current_role,\n",
        "        'total_experience': total_exp,\n",
        "        'current_tenure': current_tenure,\n",
        "        'certification_count': certs,\n",
        "        'publication_count': pubs,\n",
        "        'skill_density': density,\n",
        "        'job_count': job_count,\n",
        "        'skills': skills\n",
        "    }\n",
        "\n",
        "# ---------- Person Name Extraction ----------\n",
        "blacklist = {\n",
        "    \"curriculum\", \"resume\", \"btech\", \"b.e\", \"be\", \"mba\", \"mca\", \"mtech\", \"cse\", \"Curriculum Vitae\",\"Puc \",\"Driven Decision\",\"Email\",\"Education Post\",\n",
        "    \"ece\", \"eee\",\"internal\",\"experience\",\"Krishna University\",\"Internal Experience\t\",\"Pgdm E\",\"Nga Vas\",\"Relationship Building\",\"Skills \",\n",
        "    \"civil\", \"mech\", \"ece\", \"student\", \"intern\", \"fresher\", \"developer\", \"engineer\", \"job\",\"university\",\"woolf\",\"University Of Pune\",\n",
        "    \"Tamil Nadu\",\"Cyber Security\",\"Microchip Company\",\"J A\",\"Hackerank Httpswww Hackerrank Rakeshtm\t\",\" Business\",\"Tamil Nadu\",\"Newresume\",\n",
        "    \"file\", \"document\", \"doc\", \"new\", \"final\", \"latest\", \"updated\", \"resume1\", \"resume2\",\"Navi Mumbai\",\"Threat Analysis\",\"Race Job\",\n",
        "    \"Pre University College\",\"C O\",\"Academic Counsellor\",\"Documentation Presentation\",\n",
        "}\n",
        "\n",
        "def extract_person_name(resume_text, resume_path=None, email=None, parsed_name=None):\n",
        "    def clean_name(name):\n",
        "        name = name.lower().strip()\n",
        "        name = re.sub(r'[^a-z ]+', '', name)\n",
        "        name = re.sub(r'\\s+', ' ', name)\n",
        "        return name.title()\n",
        "\n",
        "    if parsed_name:\n",
        "        name = clean_name(parsed_name)\n",
        "        if name.lower() not in blacklist and 1 <= len(name.split()) <= 4:\n",
        "            return name\n",
        "\n",
        "    if email:\n",
        "        username = email.split(\"@\")[0]\n",
        "        name = clean_name(username.replace(\".\", \" \").replace(\"_\", \" \").replace(\"-\", \" \"))\n",
        "        parts = [p for p in name.split() if p.lower() not in blacklist and not any(char.isdigit() for char in p)]\n",
        "        if 1 <= len(parts) <= 3:\n",
        "            return \" \".join([p.title() for p in parts])\n",
        "\n",
        "    # NEW LOGIC: Look for a plausible name in the first 10 lines of resume\n",
        "    header_lines = resume_text.strip().split('\\n')[:10]\n",
        "    for line in header_lines:\n",
        "        name = clean_name(line)\n",
        "        if name.lower() not in blacklist and 1 <= len(name.split()) <= 4 and len(name) > 2:\n",
        "            return name\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------- AHP Fitment Score Calculation ----------\n",
        "def compute_ahp_fitment_score(resume_features, jd_features, weights):\n",
        "    resume_skills = set([s.lower() for s in resume_features.get('skills', [])])\n",
        "    expected_skills = set([s.lower() for s in jd_features.get('expected_skills', [])])\n",
        "    S = len(resume_skills & expected_skills) / len(expected_skills) if expected_skills else 0\n",
        "    C = 1 if jd_features['cert_required'] and resume_features['certification_count'] > 0 else 0\n",
        "    P = 1 if jd_features['pub_required'] and resume_features['publication_count'] > 0 else 0\n",
        "    E = min(resume_features['total_experience'] / jd_features['min_experience'], 1) if jd_features['min_experience'] > 0 else 1\n",
        "    T = min(resume_features.get('current_tenure', 0) / 1, 1)\n",
        "    R = SequenceMatcher(None, resume_features.get('current_role', '').lower(), jd_features.get('target_role', '').lower()).ratio()\n",
        "    score = (\n",
        "        weights['experience'] * E +\n",
        "        weights['certification'] * C +\n",
        "        weights['tenure'] * T +\n",
        "        weights['publication'] * P +\n",
        "        weights['skill_density'] * S +\n",
        "        weights['role_match'] * R\n",
        "    )\n",
        "    return int(round(score * 100))\n",
        "\n",
        "def classify_fit(score):\n",
        "    if score < 31:\n",
        "        return \"Low Fit\"\n",
        "    elif score < 71:\n",
        "        return \"Medium Fit\"\n",
        "    else:\n",
        "        return \"Fit\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVJO93RYDmoP",
        "outputId": "0cbfd960-a319-4e89-9e32-f8d7a877ab19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing extractors.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile db_utils.py\n",
        "from app_config import conn\n",
        "\n",
        "def save_to_db(data):\n",
        "    cursor = conn.cursor()\n",
        "    for row in data:\n",
        "        # Normalize keys to match DB column names\n",
        "        row[\"certification_count\"] = row.pop(\"certifications\", 0)\n",
        "        row[\"publication_count\"] = row.pop(\"publications\", 0)\n",
        "\n",
        "        cursor.execute('''\n",
        "        UPDATE parsed_resumes\n",
        "        SET\n",
        "            person_name = ?,\n",
        "            current_role = ?,\n",
        "            total_experience = ?,\n",
        "            current_tenure = ?,\n",
        "            certification_count = ?,\n",
        "            publication_count = ?,\n",
        "            skill_density = ?,\n",
        "            fitment_score = ?,\n",
        "            fitment_level = ?\n",
        "        WHERE LOWER(TRIM(REPLACE(resume_path, '\\\\', '/'))) LIKE ?\n",
        "        ''', (\n",
        "            row[\"person_name\"], row[\"current_role\"],\n",
        "            row[\"total_experience\"], row[\"current_tenure\"],\n",
        "            row[\"certification_count\"], row[\"publication_count\"], row[\"skill_density\"],\n",
        "            row[\"fitment_score\"], row[\"fitment_level\"],\n",
        "            f'%/{row[\"file_name\"].lower()}'\n",
        "        ))\n",
        "    conn.commit()\n",
        "\n",
        "def load_from_db(filenames):\n",
        "    import pandas as pd\n",
        "    cursor = conn.cursor()\n",
        "    placeholders = ','.join(['?'] * len(filenames))\n",
        "    normalized_filenames = [f.lower() for f in filenames]\n",
        "\n",
        "    if len(filenames) == 1:\n",
        "        query = \"\"\"\n",
        "        SELECT * FROM parsed_resumes\n",
        "        WHERE LOWER(REPLACE(resume_path, '\\\\\\\\', '/')) LIKE ?\n",
        "        \"\"\"\n",
        "        params = [f'%/{normalized_filenames[0]}']\n",
        "    else:\n",
        "        conditions = \" OR \".join(\n",
        "            [\"LOWER(REPLACE(resume_path, '\\\\\\\\', '/')) LIKE ?\" for _ in filenames]\n",
        "        )\n",
        "        query = f\"\"\"\n",
        "        SELECT * FROM parsed_resumes\n",
        "        WHERE {conditions}\n",
        "        \"\"\"\n",
        "        params = [f'%/{f}' for f in normalized_filenames]\n",
        "\n",
        "    df = pd.read_sql_query(query, conn, params=params)\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbP9aJWdERB-",
        "outputId": "15066a4f-47f0-402c-b97c-dfd74b4c7e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing db_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ui_layout.py\n",
        "from shiny import ui\n",
        "from app_config import col_map\n",
        "\n",
        "app_ui = ui.page_fluid(\n",
        "    ui.navset_tab(\n",
        "        ui.nav_panel(\"üìä Dashboard\",\n",
        "            ui.output_ui(\"dashboard_text_summary\"),\n",
        "            ui.h4(\"Shortlisted Candidates\"),\n",
        "            ui.layout_columns(\n",
        "                ui.div(\n",
        "                    ui.input_checkbox_group(\n",
        "                        \"shortlist_columns\",\n",
        "                        \"Columns to show:\",\n",
        "                        choices=col_map,\n",
        "                        selected=[\"person_name\", \"fitment_score\"]\n",
        "                    ),\n",
        "                    ui.download_button(\"download_shortlist\", \"Download Shortlist as CSV\"),\n",
        "                    style=\"margin-bottom: 1em;\"\n",
        "                ),\n",
        "                ui.div(\n",
        "                    ui.output_data_frame(\"shortlist_table\"),\n",
        "                    style=\"width:100%\"\n",
        "                ),\n",
        "                col_widths=[3, 9]\n",
        "            )\n",
        "        ),\n",
        "        ui.nav_menu(\"üóÉÔ∏è Data Mart\",\n",
        "            ui.nav_panel(\"Data Upload\",\n",
        "                ui.input_file(\"jd_upload\", \"Upload JDs (.docx)\", multiple=True, accept=[\".docx\"]),\n",
        "                ui.input_file(\"resume_upload\", \"Upload Resumes (.pdf, .docx)\", multiple=True, accept=[\".pdf\", \".docx\"]),\n",
        "                ui.output_text_verbatim(\"upload_status\"),\n",
        "                ui.output_ui(\"jd_selector_view\")\n",
        "            ),\n",
        "            ui.nav_panel(\"Data View\",\n",
        "                ui.output_data_frame(\"dataview_table\")\n",
        "            )\n",
        "        ),\n",
        "        ui.nav_menu(\"‚öñÔ∏è Fitment\",\n",
        "            ui.nav_panel(\"Prioritization (AHP)\",\n",
        "                ui.layout_sidebar(\n",
        "                    ui.sidebar(\n",
        "                        ui.h4(\"Adjust AHP Weights\"),\n",
        "                        ui.input_slider(\"weight_experience\", \"Experience Weight\", 0, 10, 5),\n",
        "                        ui.input_slider(\"weight_skill\", \"Skill Weight\", 0, 10, 5),\n",
        "                        ui.input_slider(\"weight_tenure\", \"Tenure Weight\", 0, 10, 5),\n",
        "                        ui.input_slider(\"weight_cert\", \"Certification Weight\", 0, 10, 5),\n",
        "                        ui.input_slider(\"weight_pub\", \"Publication Weight\", 0, 10, 5),\n",
        "                        ui.input_slider(\"weight_role\", \"Role Match Weight\", 0, 10, 5),\n",
        "                        ui.input_action_button(\"apply_weights\", \"Apply Weights\"),\n",
        "                        ui.input_action_button(\"reset_weights\", \"Reset Weights\", class_=\"btn-danger\")\n",
        "                    ),\n",
        "                    ui.output_data_frame(\"fitment_ahp_table\")\n",
        "                )\n",
        "            ),\n",
        "            ui.nav_panel(\"Parser\",\n",
        "                ui.h4(\"Extracted Resume Data\"),\n",
        "                ui.output_data_frame(\"parser_table\")\n",
        "            ),\n",
        "            ui.nav_panel(\"Fitment Score\",\n",
        "                ui.input_select(\"fitment_filter_score\", \"Filter by Fitment:\", {\n",
        "                    \"All\": \"All\",\n",
        "                    \"Low Fit\": \"Low Fit\",\n",
        "                    \"Medium Fit\": \"Medium Fit\",\n",
        "                    \"Fit\": \"Fit\"\n",
        "                }, selected=\"All\"),\n",
        "                ui.output_data_frame(\"fitment_score_table\")\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABduzuKfEYM-",
        "outputId": "16530e78-16e0-4a25-8fc0-176d0c522339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ui_layout.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main_app.py\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "\n",
        "from shiny import App, reactive, render, ui\n",
        "\n",
        "from app_config import default_ahp_weights, col_map\n",
        "from ui_layout import app_ui\n",
        "from extractors import (\n",
        "    extract_text_from_docx, extract_text_from_pdf,\n",
        "    extract_resume_features_combined as extract_resume_features,\n",
        "    extract_jd_features, compute_ahp_fitment_score as compute_ahp_score,\n",
        "    classify_fit, extract_person_name\n",
        ")\n",
        "\n",
        "from db_utils import save_to_db\n",
        "\n",
        "DB_PATH = \"/content/sample_data/Resumeparser.db\"\n",
        "\n",
        "jd_store = reactive.Value([])\n",
        "resume_store = reactive.Value([])\n",
        "ahp_weights = reactive.Value(default_ahp_weights.copy())\n",
        "features_to_normalize = [\n",
        "    \"total_experience\", \"current_tenure\",\n",
        "    \"certification_count\", \"publication_count\", \"skill_density\"\n",
        "]\n",
        "\n",
        "def fetch_features_from_db(filename):\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    df = pd.read_sql_query(\n",
        "        \"SELECT * FROM parsed_resumes WHERE resume_path LIKE ?\",\n",
        "        conn, params=(f\"%{filename}\",)\n",
        "    )\n",
        "    conn.close()\n",
        "    return df.iloc[0].to_dict() if not df.empty else None\n",
        "\n",
        "def normalize_df(df):\n",
        "    for col in features_to_normalize:\n",
        "        max_val = df[col].max()\n",
        "        if max_val > 0:\n",
        "            df[f\"norm_{col}\"] = (df[col] / max_val >= 0.5).astype(int)\n",
        "        else:\n",
        "            df[f\"norm_{col}\"] = 0\n",
        "    return df\n",
        "\n",
        "def server(input, output, session):\n",
        "\n",
        "    @reactive.Effect\n",
        "    def _():\n",
        "        if input.jd_upload():\n",
        "            jd_store.set(input.jd_upload())\n",
        "        if input.resume_upload():\n",
        "            resume_store.set(input.resume_upload())\n",
        "\n",
        "    @output\n",
        "    @render.text\n",
        "    def upload_status():\n",
        "        return f\"{len(jd_store.get())} JD(s) uploaded, {len(resume_store.get())} resume(s) uploaded\"\n",
        "\n",
        "    @output\n",
        "    @render.ui\n",
        "    def jd_selector_view():\n",
        "        jds = jd_store.get()\n",
        "        choices = {f[\"name\"]: f[\"name\"] for f in jds} if jds else {}\n",
        "        return ui.input_select(\"jd_select_view\", \"Select JD:\", choices=choices)\n",
        "\n",
        "    @reactive.Calc\n",
        "    def selected_jd_features():\n",
        "        jds = jd_store.get()\n",
        "        selected_name = input.jd_select_view()\n",
        "        if not selected_name:\n",
        "            return None\n",
        "        jd_file = next((jd for jd in jds if jd[\"name\"] == selected_name), None)\n",
        "        if not jd_file:\n",
        "            return None\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".docx\") as tmp_jd:\n",
        "            with open(jd_file[\"datapath\"], \"rb\") as f:\n",
        "                tmp_jd.write(f.read())\n",
        "            jd_text = extract_text_from_docx(tmp_jd.name)\n",
        "            return extract_jd_features(jd_text)\n",
        "\n",
        "    @reactive.Calc\n",
        "    def dashboard_results():\n",
        "        resumes = resume_store.get()\n",
        "        jd_features = selected_jd_features()\n",
        "        if not resumes or not jd_features:\n",
        "            df = pd.DataFrame()\n",
        "            # But still save empty as backup for robustness\n",
        "            df.to_csv(\"dashboard_shortlist_latest.csv\", index=False)\n",
        "            return df\n",
        "\n",
        "        weights = ahp_weights.get()\n",
        "        rows = []\n",
        "        for resume in resumes:\n",
        "            filename = resume[\"name\"].lower()\n",
        "            db_features = fetch_features_from_db(filename)\n",
        "            if db_features:\n",
        "                person_name = db_features.get(\"person_name\") or os.path.splitext(filename)[0]\n",
        "                score = compute_ahp_score(db_features, jd_features, weights)\n",
        "                fit = classify_fit(score)\n",
        "                row = {\n",
        "                    **db_features,\n",
        "                    \"file_name\": filename,\n",
        "                    \"person_name\": person_name,\n",
        "                    \"fitment_score\": score,\n",
        "                    \"fitment_level\": fit,\n",
        "                }\n",
        "            else:\n",
        "                ext = os.path.splitext(resume[\"name\"])[-1].lower()\n",
        "                suffix = \".pdf\" if ext == \".pdf\" else \".docx\"\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
        "                    with open(resume[\"datapath\"], \"rb\") as f:\n",
        "                        tmp.write(f.read())\n",
        "                    text = extract_text_from_pdf(tmp.name) if suffix == \".pdf\" else extract_text_from_docx(tmp.name)\n",
        "                    features = extract_resume_features(tmp.name, text)\n",
        "                    person_name = extract_person_name(resume_text=text, resume_path=tmp.name)\n",
        "                    if not person_name or person_name.lower() == \"unknown\":\n",
        "                        # fallback to file name\n",
        "                        person_name = os.path.splitext(filename)[0].replace(\"_\", \" \").title()\n",
        "                    score = compute_ahp_score(features, jd_features, weights)\n",
        "                    fit = classify_fit(score)\n",
        "                    row = {\n",
        "                        **features,\n",
        "                        \"file_name\": filename,\n",
        "                        \"person_name\": person_name,\n",
        "                        \"fitment_score\": score,\n",
        "                        \"fitment_level\": fit,\n",
        "                    }\n",
        "            rows.append(row)\n",
        "        df = pd.DataFrame(rows)\n",
        "        for col in features_to_normalize:\n",
        "            max_val = df[col].max() if not df.empty else 0\n",
        "            if max_val > 0:\n",
        "                df[f\"norm_{col}\"] = (df[col] / max_val >= 0.5).astype(int)\n",
        "            else:\n",
        "                df[f\"norm_{col}\"] = 0\n",
        "\n",
        "        # Save exact shortlist currently viewing on dashboard in CSV as backup\n",
        "        try:\n",
        "            df.to_csv(\"dashboard_shortlist_latest.csv\", index=False)\n",
        "        except Exception as e:\n",
        "            print(\"Could not save current shortlist as CSV:\", e)\n",
        "\n",
        "        #  Save updated rows to DB\n",
        "        try:\n",
        "            save_to_db(rows)\n",
        "        except Exception as e:\n",
        "            print(\"Could not save updated rows to DB:\", e)\n",
        "\n",
        "        return df\n",
        "\n",
        "    @output\n",
        "    @render.ui\n",
        "    def dashboard_text_summary():\n",
        "        df = dashboard_results()\n",
        "        if df.empty:\n",
        "            return ui.pre(\"No data available.\")\n",
        "\n",
        "        return ui.div(\n",
        "            ui.pre(\n",
        "                f\"Total Resumes: {len(df)}\\n\"\n",
        "                f\"Experience > 1 year: {df['norm_total_experience'].sum()}\\n\"\n",
        "                f\"Tenure > 1 year: {df['norm_current_tenure'].sum()}\\n\"\n",
        "                f\"Certifications present: {df['norm_certification_count'].sum()}\\n\"\n",
        "                f\"Publications present: {df['norm_publication_count'].sum()}\\n\"\n",
        "                f\"Skill keywords present: {df['norm_skill_density'].sum()}\\n\"\n",
        "                f\"Shortlisted (Fit): {(df['fitment_level'] == 'Fit').sum()}\"\n",
        "            ),\n",
        "            ui.input_radio_buttons(\n",
        "                \"shortlist_filter_option\",\n",
        "                \"Show Candidates by:\",\n",
        "                {\n",
        "                    \"All\": \"All\", \"Experience\": \"Experience\", \"Tenure\": \"Tenure\",\n",
        "                    \"Certification\": \"Certification\", \"Publication\": \"Publication\", \"Skills\": \"Skills\"\n",
        "                },\n",
        "                selected=\"All\", inline=True\n",
        "            )\n",
        "        )\n",
        "\n",
        "    @output\n",
        "    @render.data_frame\n",
        "    def shortlist_table():\n",
        "        df = dashboard_results()\n",
        "        if df.empty:\n",
        "            return pd.DataFrame()\n",
        "        option = input.shortlist_filter_option()\n",
        "        if option == \"Experience\":\n",
        "            df = df[df[\"norm_total_experience\"] == 1]\n",
        "        elif option == \"Tenure\":\n",
        "            df = df[df[\"norm_current_tenure\"] == 1]\n",
        "        elif option == \"Certification\":\n",
        "            df = df[df[\"norm_certification_count\"] == 1]\n",
        "        elif option == \"Publication\":\n",
        "            df = df[df[\"norm_publication_count\"] == 1]\n",
        "        elif option == \"Skills\":\n",
        "            df = df[df[\"norm_skill_density\"] == 1]\n",
        "        selected_cols = input.shortlist_columns() or [\"person_name\", \"fitment_score\"]\n",
        "        return df[[col for col in selected_cols if col in df.columns]]\n",
        "\n",
        "    @output\n",
        "    @render.download(filename=\"dashboard_shortlist.csv\")\n",
        "    def download_shortlist():\n",
        "        # Download exactly what is currently showing in shortlist_table\n",
        "        def writer(file):\n",
        "            try:\n",
        "                df = dashboard_results()\n",
        "                if df.empty:\n",
        "                    file.write(\"No data available.\")\n",
        "                    return\n",
        "\n",
        "                # Apply same filter as shortlist_table\n",
        "                option = input.shortlist_filter_option()\n",
        "                if option == \"Experience\":\n",
        "                    df = df[df[\"norm_total_experience\"] == 1]\n",
        "                elif option == \"Tenure\":\n",
        "                    df = df[df[\"norm_current_tenure\"] == 1]\n",
        "                elif option == \"Certification\":\n",
        "                    df = df[df[\"norm_certification_count\"] == 1]\n",
        "                elif option == \"Publication\":\n",
        "                    df = df[df[\"norm_publication_count\"] == 1]\n",
        "                elif option == \"Skills\":\n",
        "                    df = df[df[\"norm_skill_density\"] == 1]\n",
        "\n",
        "                selected_cols = input.shortlist_columns() or [\"person_name\", \"fitment_score\"]\n",
        "                filtered_df = df[[col for col in selected_cols if col in df.columns]].copy()\n",
        "                # Optional pretty col names\n",
        "                rename_map = {col: col_map.get(col, col) for col in filtered_df.columns}\n",
        "                filtered_df.rename(columns=rename_map, inplace=True)\n",
        "                # Write actual CSV\n",
        "                filtered_df.to_csv(file, index=False)\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                file.write(\"Download failed due to an internal error.\")\n",
        "        return writer\n",
        "\n",
        "    @reactive.Calc\n",
        "    def dataview_table_data():\n",
        "        resumes = resume_store.get()\n",
        "        jd_features = selected_jd_features()\n",
        "        if not resumes or not jd_features:\n",
        "            return pd.DataFrame(columns=[\n",
        "                \"file_name\", \"person_name\", \"current_role\",\n",
        "                \"norm_total_experience\", \"norm_current_tenure\",\n",
        "                \"norm_certification_count\", \"norm_publication_count\",\n",
        "                \"norm_skill_density\", \"fitment_score\", \"fitment_level\"\n",
        "            ])\n",
        "        weights = ahp_weights.get()\n",
        "        results = []\n",
        "        for resume in resumes:\n",
        "            filename = resume[\"name\"]\n",
        "            db_features = fetch_features_from_db(filename)\n",
        "            if db_features:\n",
        "                # Known/resume in DB\n",
        "                score = compute_ahp_score(db_features, jd_features, weights)\n",
        "                fit = classify_fit(score)\n",
        "                row = {\n",
        "                    **db_features,\n",
        "                    \"file_name\": filename,\n",
        "                    \"fitment_score\": score,\n",
        "                    \"fitment_level\": fit,\n",
        "                }\n",
        "                # Ensure person_name is present, fallback to filename\n",
        "                if not row.get(\"person_name\") or row[\"person_name\"].lower() == \"unknown\":\n",
        "                    row[\"person_name\"] = os.path.splitext(filename)[0].replace(\"_\", \" \").title()\n",
        "                results.append(row)\n",
        "            else:\n",
        "                # New resume: extract, score, save, show\n",
        "                ext = os.path.splitext(filename)[-1].lower()\n",
        "                suffix = \".pdf\" if ext == \".pdf\" else \".docx\"\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
        "                    with open(resume[\"datapath\"], \"rb\") as f:\n",
        "                        tmp.write(f.read())\n",
        "                    text = extract_text_from_pdf(tmp.name) if suffix == \".pdf\" else extract_text_from_docx(tmp.name)\n",
        "                    features = extract_resume_features(tmp.name, text)\n",
        "                    # Extract person_name from resume text, fallback to filename\n",
        "                    person_name = extract_person_name(resume_text=text)\n",
        "                    if not person_name or person_name.lower() == \"unknown\":\n",
        "                        # fallback\n",
        "                        person_name = os.path.splitext(filename)[0].replace(\"_\", \" \").title()\n",
        "\n",
        "                    score = compute_ahp_score(features, jd_features, weights)\n",
        "                    fit = classify_fit(score)\n",
        "                    row = {\n",
        "                        **features,\n",
        "                        \"file_name\": filename,\n",
        "                        \"person_name\": person_name,\n",
        "                        \"fitment_score\": score,\n",
        "                        \"fitment_level\": fit,\n",
        "                    }\n",
        "                    # Save for DB so future runs are fast\n",
        "                    save_to_db([row])\n",
        "                    results.append(row)\n",
        "        # Build DataFrame and normalize\n",
        "        df = pd.DataFrame(results)\n",
        "        for col in features_to_normalize:\n",
        "            max_val = df[col].max() if not df.empty else 0\n",
        "            if max_val > 0:\n",
        "                df[f\"norm_{col}\"] = (df[col] / max_val >= 0.5).astype(int)\n",
        "            else:\n",
        "                df[f\"norm_{col}\"] = 0\n",
        "        # Always return DF with all columns present\n",
        "        columns_out = [\n",
        "            \"file_name\", \"person_name\", \"current_role\",\n",
        "            \"norm_total_experience\", \"norm_current_tenure\",\n",
        "            \"norm_certification_count\", \"norm_publication_count\",\n",
        "            \"norm_skill_density\", \"fitment_score\", \"fitment_level\"\n",
        "        ]\n",
        "        # Add any missing columns as empty/zeros\n",
        "        for col in columns_out:\n",
        "            if col not in df.columns:\n",
        "                df[col] = \"\"\n",
        "        return df[columns_out]\n",
        "\n",
        "    @output\n",
        "    @render.data_frame\n",
        "    def dataview_table():\n",
        "        df = dataview_table_data()\n",
        "        if df.empty:\n",
        "            return pd.DataFrame(columns=[\n",
        "                \"file_name\", \"person_name\", \"current_role\",\n",
        "                \"norm_total_experience\", \"norm_current_tenure\",\n",
        "                \"norm_certification_count\", \"norm_publication_count\",\n",
        "                \"norm_skill_density\", \"fitment_score\", \"fitment_level\"\n",
        "            ])\n",
        "        return df[[\n",
        "            \"file_name\", \"person_name\", \"current_role\",\n",
        "            \"norm_total_experience\", \"norm_current_tenure\",\n",
        "            \"norm_certification_count\", \"norm_publication_count\",\n",
        "            \"norm_skill_density\", \"fitment_score\", \"fitment_level\"\n",
        "        ]]\n",
        "\n",
        "\n",
        "    @output\n",
        "    @render.data_frame\n",
        "    def fitment_ahp_table():\n",
        "        df = dashboard_results()\n",
        "        return df[[\"person_name\", \"fitment_score\", \"fitment_level\"]] if not df.empty else pd.DataFrame()\n",
        "\n",
        "    @reactive.Effect\n",
        "    def _():\n",
        "        if input.apply_weights():\n",
        "            total = (\n",
        "                input.weight_experience() + input.weight_skill() +\n",
        "                input.weight_tenure() + input.weight_cert() +\n",
        "                input.weight_pub() + input.weight_role()\n",
        "            )\n",
        "            if total > 0:\n",
        "                new_weights = {\n",
        "                    \"experience\": input.weight_experience() / total,\n",
        "                    \"skill_density\": input.weight_skill() / total,\n",
        "                    \"tenure\": input.weight_tenure() / total,\n",
        "                    \"certification\": input.weight_cert() / total,\n",
        "                    \"publication\": input.weight_pub() / total,\n",
        "                    \"role_match\": input.weight_role() / total\n",
        "                }\n",
        "                ahp_weights.set(new_weights)\n",
        "        if input.reset_weights():\n",
        "            ahp_weights.set(default_ahp_weights.copy())\n",
        "\n",
        "    @output\n",
        "    @render.data_frame\n",
        "    def parser_table():\n",
        "        df = dashboard_results()\n",
        "        if df.empty:\n",
        "            return pd.DataFrame()\n",
        "        cols = [\n",
        "            \"file_name\", \"person_name\", \"current_role\", \"total_experience\",\n",
        "            \"current_tenure\", \"certification_count\", \"publication_count\", \"skill_density\"\n",
        "        ]\n",
        "        return df[cols]\n",
        "\n",
        "    @reactive.Calc\n",
        "    def fitment_score_table_data():\n",
        "        df = dashboard_results()\n",
        "        level = input.fitment_filter_score()\n",
        "        return df if level == \"All\" else df[df[\"fitment_level\"] == level]\n",
        "\n",
        "    @output\n",
        "    @render.data_frame\n",
        "    def fitment_score_table():\n",
        "        df = fitment_score_table_data()\n",
        "        if not df.empty:\n",
        "            return df[[\"person_name\", \"fitment_score\", \"fitment_level\"]]\n",
        "        else:\n",
        "            return pd.DataFrame(columns=[\"person_name\", \"fitment_score\", \"fitment_level\"])\n",
        "\n",
        "app = App(app_ui, server)\n"
      ],
      "metadata": {
        "id": "Qbx96LeSEfFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc831d4e-5088-4b61-cad6-2aeeef98dc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "from shiny import App, reactive, render, ui\n",
        "from shiny.types import FileInfo\n",
        "from app_config import default_ahp_weights, col_map\n",
        "from extractors import classify_fit\n",
        "\n",
        "from ui_layout import app_ui\n",
        "from extractors import (\n",
        "    extract_text_from_docx, extract_text_from_pdf,\n",
        "    extract_resume_features_combined as extract_resume_features,\n",
        "    extract_jd_features, compute_ahp_fitment_score as compute_ahp_score,\n",
        "    classify_fit   # Add this line\n",
        ")\n",
        "from db_utils import save_to_db\n"
      ],
      "metadata": {
        "id": "bXZeIWNYE9t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Ensure Shiny app is saved (skip if already saved)\n",
        "# with open(\"main_app.py\", \"w\") as f:\n",
        "#     f.write(shiny_code)\n",
        "\n",
        "#  Install necessary packages\n",
        "!pip install -q shiny pyresparser python-docx PyMuPDF ahpy\n",
        "\n",
        "#  Kill any previous Shiny or Cloudflare tunnels\n",
        "!pkill -f shiny || true\n",
        "!pkill -f cloudflared || true\n",
        "\n",
        "#  Download and set up Cloudflare tunnel binary\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "!mv cloudflared /usr/local/bin/cloudflared\n",
        "\n",
        "#  Clear old tunnel logs if any\n",
        "!rm -f url.txt shiny_log.txt\n",
        "\n",
        "#  Start Shiny app and cloudflared tunnel\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "!shiny run --host 0.0.0.0 --port 8084 main_app.py > shiny_log.txt 2>&1 &\n",
        "!nohup cloudflared tunnel --url http://localhost:8084 --logfile url.txt > /dev/null 2>&1 &\n",
        "\n",
        "#  Wait for tunnel to establish\n",
        "time.sleep(8)\n",
        "\n",
        "#  Display public URL\n",
        "print(\"üîó Public Shiny App Link:\")\n",
        "!grep -o 'https://[^ ]*\\.trycloudflare\\.com' url.txt | tail -n 1\n",
        "\n",
        "#  Show last few log lines for debugging\n",
        "print(\"\\nüìÑ Last 10 lines of shiny_log.txt:\")\n",
        "!tail -n 10 shiny_log.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKza68iXCvtE",
        "outputId": "c9a3940b-be0d-41c5-b073-8161efe5adc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "^C\n",
            "üîó Public Shiny App Link:\n",
            "https://officers-anxiety-bufing-locked.trycloudflare.com\n",
            "\n",
            "üìÑ Last 10 lines of shiny_log.txt:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/dashboard_shortlist_latest.csv')\n"
      ],
      "metadata": {
        "id": "qj_9WC7pwAiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}